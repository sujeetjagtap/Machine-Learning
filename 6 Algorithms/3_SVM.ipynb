{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4fc12b",
   "metadata": {},
   "source": [
    "# Support Vector Machine: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509cf4",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a supervised learning algorithm that can be used for both prediction of a binary variable (classification) and quantitative variable (regression problems) even if it is primarily used for classification problems. The goal of SVM is to create a hyperplane that linearly divide n-dimensional data points in two components by searching for an optimal margin that correctly segregate the data into different classes and at the same time be separated as much as possible from all the observations. In\n",
    "addition to linear classification, it is also possible to compute a non-linear classification using what we call the kernel trick (kernel function) that maps inputs into high dimensional feature spaces. The kernel function adapted to specific problems allows a real flexibility to adapt to different situations. SVM allows to create a classifier, or a discrimination function, that we can generalize and apply for prediction such as in image classification, diagnostics, genomic sequences, or drug discovery.  SVM was developed at\n",
    "AT&T Bell Laboratories by Vladimir Vapnik and colleagues. To select the optimal hyperplane amongst many hyperplanes that might classify our data, we select the one that has the largest margin or in another words that represents the largest separation between the different classes. It is an optimization problem under constraints where the distance between the nearest data point and the optimal hyperplane (on each side) is maximized. The hyperplane is then called the maximum-margin hyperplane allowing us to create a\n",
    "maximum-margin classifier. The closest data-points are known as support vectors and margin is an area which generally do not contains any data points. If the optimal hyperplane is too close to data points and the margin too small, it will be difficult to predict new data and the model will fail to generalize well. In non-linear cases, we will need to introduce a kernel function to search for nonlinear separating surfaces. The method will induce a nonlinear transformation of our dataset towards an intermediate space that we call feature space of higher dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60875852",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad38ed94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM_linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.801471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.801471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.801471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.801471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation mean</th>\n",
       "      <td>0.934126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation std</th>\n",
       "      <td>0.002855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SVM_linear\n",
       "Accuracy                 0.801471\n",
       "Precision                0.801471\n",
       "Recall                   0.801471\n",
       "F1 Score                 0.801471\n",
       "Cross-validation mean    0.934126\n",
       "Cross-validation std     0.002855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data: Capture the dataset in Python using Pandas DataFrame\n",
    "csv_data = '../data/datasets/neurons_binary.csv'\n",
    "df = pd.read_csv(csv_data, delimiter=';')\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "\n",
    "# Model \n",
    "model=svm.SVC(kernel='linear')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(model, X_train, y_train, cv=5).mean(), cross_val_score(model, X_train, y_train, cv=5).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns={'SVM_linear'})\n",
    "\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e674c4",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "177ccc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM_rbf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.860115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.860115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.860115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.860115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation mean</th>\n",
       "      <td>0.902377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation std</th>\n",
       "      <td>0.002033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SVM_rbf\n",
       "Accuracy               0.860115\n",
       "Precision              0.860115\n",
       "Recall                 0.860115\n",
       "F1 Score               0.860115\n",
       "Cross-validation mean  0.902377\n",
       "Cross-validation std   0.002033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data: Capture the dataset in Python using Pandas DataFrame\n",
    "csv_data = '../data/datasets/neurons.csv'\n",
    "df = pd.read_csv(csv_data, delimiter=';')\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "\n",
    "# Model \n",
    "model=svm.SVC(kernel='rbf')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(model, X_train, y_train, cv=5).mean(), cross_val_score(model, X_train, y_train, cv=5).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns={'SVM_rbf'})\n",
    "\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dde08",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39beb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM_sigmoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.755918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.755918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.755918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.755918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation mean</th>\n",
       "      <td>0.728386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation std</th>\n",
       "      <td>0.010850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SVM_sigmoid\n",
       "Accuracy                  0.755918\n",
       "Precision                 0.755918\n",
       "Recall                    0.755918\n",
       "F1 Score                  0.755918\n",
       "Cross-validation mean     0.728386\n",
       "Cross-validation std      0.010850"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data: Capture the dataset in Python using Pandas DataFrame\n",
    "csv_data = '../data/datasets/neurons.csv'\n",
    "df = pd.read_csv(csv_data, delimiter=';')\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "\n",
    "# Model \n",
    "model=svm.SVC(kernel='sigmoid')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(model, X_train, y_train, cv=5).mean(), cross_val_score(model, X_train, y_train, cv=5).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns={'SVM_sigmoid'})\n",
    "\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74effe86",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e1af3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM_poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.727762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.727762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.727762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.727762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation mean</th>\n",
       "      <td>0.670359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-validation std</th>\n",
       "      <td>0.017285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SVM_poly\n",
       "Accuracy               0.727762\n",
       "Precision              0.727762\n",
       "Recall                 0.727762\n",
       "F1 Score               0.727762\n",
       "Cross-validation mean  0.670359\n",
       "Cross-validation std   0.017285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data: Capture the dataset in Python using Pandas DataFrame\n",
    "csv_data = '../data/datasets/neurons.csv'\n",
    "df = pd.read_csv(csv_data, delimiter=';')\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "\n",
    "# Model \n",
    "model=svm.SVC(kernel='poly')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(model, X_train, y_train, cv=5).mean(), cross_val_score(model, X_train, y_train, cv=5).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns={'SVM_poly'})\n",
    "\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805d2b6",
   "metadata": {},
   "source": [
    "# Support Vector Machine for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54d3aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5576, 22300]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/4vnghq3579l_6xbflfl1hk180000gn/T/ipykernel_36741/2593161799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5576, 22300]"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data: Capture the dataset in Python using Pandas DataFrame\n",
    "csv_data = '../data/datasets/neurons.csv'\n",
    "df = pd.read_csv(csv_data, delimiter=';')\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# define PCA. n_components: number of principal components we want to keep\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# transform data\n",
    "X_train = pca.fit_transform(X_test)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "\n",
    "# Model \n",
    "model=svm.SVC(kernel='sigmoid')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(model, X_train, y_train, cv=5).mean(), cross_val_score(model, X_train, y_train, cv=5).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns={'SVM_sigmoid'})\n",
    "\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f2b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
