{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTf0wk4A53lJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38540e36"
      },
      "source": [
        "# NEURAL NETWORK\n",
        "An hands-on approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68a38436"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "### Introduce the concept of neural networks and the goal of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "c464467e",
        "outputId": "8737f5e8-3c62-4149-82f3-55f966b09936"
      },
      "source": [
        "%%markdown\n",
        "# Building a Simple Neural Network from Scratch\n",
        "\n",
        "## Introduction to Neural Networks\n",
        "\n",
        "Neural networks are a set of algorithms, modeled after the human brain, that are designed to recognize patterns. They are particularly good at tasks like image recognition, natural language processing, and making predictions based on complex data. At their core, neural networks are composed of interconnected nodes, or \"neurons,\" organized in layers. These neurons process and transmit information, learning from data through a process called training.\n",
        "\n",
        "## Notebook Goal\n",
        "\n",
        "The goal of this notebook is to provide a hands-on introduction to the fundamental building blocks of neural networks by guiding you through the process of coding a simple neural network from scratch using Python. We will start by understanding the basic structure of a neuron, then build up to creating input, hidden, and output layers.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "For this notebook, we will be using a simple dataset to illustrate the concepts. The details of the dataset will be covered in the next section."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Building a Simple Neural Network from Scratch\n\n## Introduction to Neural Networks\n\nNeural networks are a set of algorithms, modeled after the human brain, that are designed to recognize patterns. They are particularly good at tasks like image recognition, natural language processing, and making predictions based on complex data. At their core, neural networks are composed of interconnected nodes, or \"neurons,\" organized in layers. These neurons process and transmit information, learning from data through a process called training.\n\n## Notebook Goal\n\nThe goal of this notebook is to provide a hands-on introduction to the fundamental building blocks of neural networks by guiding you through the process of coding a simple neural network from scratch using Python. We will start by understanding the basic structure of a neuron, then build up to creating input, hidden, and output layers.\n\n## Dataset\n\nFor this notebook, we will be using a simple dataset to illustrate the concepts. The details of the dataset will be covered in the next section.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40986aea"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Load and prepare a simple dataset suitable for a binary classification problem with a few features and a binary target variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0249c7f0",
        "outputId": "a0a48d19-39b0-4620-e96a-e4691572fe6d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42) # for reproducibility\n",
        "n_samples = 100\n",
        "\n",
        "# Feature 1: Random values\n",
        "feature1 = np.random.rand(n_samples) * 10\n",
        "\n",
        "# Feature 2: Values related to feature1 with some noise\n",
        "feature2 = feature1 * 0.5 + np.random.randn(n_samples) * 2\n",
        "\n",
        "# Target variable: Binary based on a simple threshold rule with some noise\n",
        "target = (feature1 + feature2 > 10).astype(int)\n",
        "# Introduce some noise in the target\n",
        "noise_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
        "target[noise_indices] = 1 - target[noise_indices]\n",
        "\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df = pd.DataFrame({'feature1': feature1, 'feature2': feature2, 'target': target})\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "display(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   feature1  feature2  target\n",
              "0  3.745401  2.046795       0\n",
              "1  9.507143  4.155557       1\n",
              "2  7.319939  3.843491       0\n",
              "3  5.986585 -0.981845       0\n",
              "4  1.560186  0.340749       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91b96148-8ad6-4dc6-baab-364dd0dbb31f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.745401</td>\n",
              "      <td>2.046795</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.507143</td>\n",
              "      <td>4.155557</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.319939</td>\n",
              "      <td>3.843491</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.986585</td>\n",
              "      <td>-0.981845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.560186</td>\n",
              "      <td>0.340749</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91b96148-8ad6-4dc6-baab-364dd0dbb31f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91b96148-8ad6-4dc6-baab-364dd0dbb31f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91b96148-8ad6-4dc6-baab-364dd0dbb31f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3345087a-f3fd-485f-bcb5-118f7d382d5a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3345087a-f3fd-485f-bcb5-118f7d382d5a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3345087a-f3fd-485f-bcb5-118f7d382d5a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.087482496186217,\n        \"min\": 1.5601864044243652,\n        \"max\": 9.50714306409916,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.50714306409916,\n          1.5601864044243652,\n          7.319939418114051\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.214764336911926,\n        \"min\": -0.9818454082166026,\n        \"max\": 4.1555568311178455,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.1555568311178455,\n          0.34074942653715873,\n          3.8434912621280297\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2b27da"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the generated dataset using a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "b56c4a53",
        "outputId": "8d6f1603-111f-40ac-a451-cc37fa6ebb8e"
      },
      "source": [
        "%%markdown\n",
        "## Dataset Description\n",
        "\n",
        "We have created a simple synthetic dataset for binary classification.\n",
        "\n",
        "*   **Features (`X`):**\n",
        "    *   `feature1`: A continuous numerical feature generated randomly.\n",
        "    *   `feature2`: A continuous numerical feature related to `feature1` with some added noise.\n",
        "*   **Target Variable (`y`):**\n",
        "    *   `target`: A binary variable (0 or 1) indicating the class. The target is primarily determined by a simple rule based on `feature1` and `feature2`, with some random noise introduced to make it slightly less linearly separable.\n",
        "\n",
        "This dataset is small and simple, making it suitable for demonstrating the basic concepts of building a neural network from scratch. The features (`X`) are stored in a pandas DataFrame, and the target variable (`y`) is stored in a pandas Series. Both are in a numerical format."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Dataset Description\n\nWe have created a simple synthetic dataset for binary classification.\n\n*   **Features (`X`):**\n    *   `feature1`: A continuous numerical feature generated randomly.\n    *   `feature2`: A continuous numerical feature related to `feature1` with some added noise.\n*   **Target Variable (`y`):**\n    *   `target`: A binary variable (0 or 1) indicating the class. The target is primarily determined by a simple rule based on `feature1` and `feature2`, with some random noise introduced to make it slightly less linearly separable.\n\nThis dataset is small and simple, making it suitable for demonstrating the basic concepts of building a neural network from scratch. The features (`X`) are stored in a pandas DataFrame, and the target variable (`y`) is stored in a pandas Series. Both are in a numerical format.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bab4b1b"
      },
      "source": [
        "## Coding a Neuron\n",
        "\n",
        "### Explain the basic structure and function of a single neuron, including inputs, weights, bias, weighted sum, and activation function using a markdown cell as the first step of the subtask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "5ea85d75",
        "outputId": "690084bb-78ca-4bf7-9995-5b94aeaa8657"
      },
      "source": [
        "%%markdown\n",
        "## The Neuron: The Basic Building Block\n",
        "\n",
        "At the heart of a neural network is the neuron, a simplified model of a biological neuron. A single neuron receives inputs, processes them, and produces an output. Let's break down its components:\n",
        "\n",
        "1.  **Inputs:** These are the values the neuron receives from the outside world or from other neurons in the network. In our case, these will be the features from our dataset (`feature1` and `feature2`).\n",
        "2.  **Weights:** Each input is associated with a weight. Weights represent the strength or importance of each input. During the training process, the network learns the optimal values for these weights.\n",
        "3.  **Bias:** The bias is an additional parameter in the neuron. It can be thought of as a threshold that the weighted sum must exceed to activate the neuron. It allows the neuron to shift the activation function output independently of the inputs.\n",
        "4.  **Weighted Sum:** The neuron calculates a weighted sum of its inputs. This is done by multiplying each input by its corresponding weight and summing up all these products. The bias term is then added to this sum.\n",
        "    *   Mathematically: $Z = (input_1 \\times weight_1) + (input_2 \\times weight_2) + ... + (input_n \\times weight_n) + bias$\n",
        "5.  **Activation Function:** The weighted sum is then passed through an activation function. The activation function introduces non-linearity into the neuron's output, which is crucial for the network to learn complex patterns. Without activation functions, a neural network would simply be a linear model, regardless of the number of layers.\n",
        "\n",
        "The output of the activation function is the output of the neuron, which can then be passed as input to other neurons in the next layer.\n",
        "\n",
        "Let's implement the weighted sum and a simple activation function."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## The Neuron: The Basic Building Block\n\nAt the heart of a neural network is the neuron, a simplified model of a biological neuron. A single neuron receives inputs, processes them, and produces an output. Let's break down its components:\n\n1.  **Inputs:** These are the values the neuron receives from the outside world or from other neurons in the network. In our case, these will be the features from our dataset (`feature1` and `feature2`).\n2.  **Weights:** Each input is associated with a weight. Weights represent the strength or importance of each input. During the training process, the network learns the optimal values for these weights.\n3.  **Bias:** The bias is an additional parameter in the neuron. It can be thought of as a threshold that the weighted sum must exceed to activate the neuron. It allows the neuron to shift the activation function output independently of the inputs.\n4.  **Weighted Sum:** The neuron calculates a weighted sum of its inputs. This is done by multiplying each input by its corresponding weight and summing up all these products. The bias term is then added to this sum.\n    *   Mathematically: $Z = (input_1 \\times weight_1) + (input_2 \\times weight_2) + ... + (input_n \\times weight_n) + bias$\n5.  **Activation Function:** The weighted sum is then passed through an activation function. The activation function introduces non-linearity into the neuron's output, which is crucial for the network to learn complex patterns. Without activation functions, a neural network would simply be a linear model, regardless of the number of layers.\n\nThe output of the activation function is the output of the neuron, which can then be passed as input to other neurons in the next layer.\n\nLet's implement the weighted sum and a simple activation function.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b0fb91"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement Python functions for calculating the weighted sum and a simple activation function (step function), and demonstrate their usage with example inputs, weights, and bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7870bc9a",
        "outputId": "a47b47f2-7239-4def-93b8-0b7ce1f899ae"
      },
      "source": [
        "# 2. Implement a Python function for calculating the weighted sum\n",
        "def weighted_sum(inputs, weights, bias):\n",
        "  \"\"\"\n",
        "  Calculates the weighted sum of inputs.\n",
        "\n",
        "  Args:\n",
        "    inputs: A list or numpy array of input values.\n",
        "    weights: A list or numpy array of weights corresponding to the inputs.\n",
        "    bias: The bias term.\n",
        "\n",
        "  Returns:\n",
        "    The weighted sum of inputs plus the bias.\n",
        "  \"\"\"\n",
        "  return np.dot(inputs, weights) + bias\n",
        "\n",
        "# 3. Implement a simple activation function (Step Function)\n",
        "def step_function(weighted_sum_output):\n",
        "  \"\"\"\n",
        "  Implements the step activation function.\n",
        "\n",
        "  Args:\n",
        "    weighted_sum_output: The output of the weighted sum.\n",
        "\n",
        "  Returns:\n",
        "    1 if the weighted sum output is non-negative, 0 otherwise.\n",
        "  \"\"\"\n",
        "  return 1 if weighted_sum_output >= 0 else 0\n",
        "\n",
        "# 4. Demonstrate the usage with example inputs, weights, and bias\n",
        "# Example inputs (e.g., features for a single data point)\n",
        "example_inputs = np.array([X['feature1'][0], X['feature2'][0]])\n",
        "\n",
        "# Example weights (randomly initialized for demonstration)\n",
        "# The number of weights should match the number of inputs\n",
        "example_weights = np.array([0.5, -0.2])\n",
        "\n",
        "# Example bias (randomly initialized for demonstration)\n",
        "example_bias = 0.1\n",
        "\n",
        "# Calculate the weighted sum\n",
        "z = weighted_sum(example_inputs, example_weights, example_bias)\n",
        "print(f\"Example Inputs: {example_inputs}\")\n",
        "print(f\"Example Weights: {example_weights}\")\n",
        "print(f\"Example Bias: {example_bias}\")\n",
        "print(f\"Weighted Sum (Z): {z}\")\n",
        "\n",
        "# Apply the activation function\n",
        "output = step_function(z)\n",
        "print(f\"Output after Step Activation: {output}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Inputs: [3.74540119 2.04679473]\n",
            "Example Weights: [ 0.5 -0.2]\n",
            "Example Bias: 0.1\n",
            "Weighted Sum (Z): 1.5633416480941815\n",
            "Output after Step Activation: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cdc83a"
      },
      "source": [
        "## Input layer\n",
        "\n",
        "### Explain the role of the input layer and the importance of data dimensions, then demonstrate checking the dimensions of the input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "fce65226",
        "outputId": "e99434bf-cea5-4f3c-e8a8-8830f55e8416"
      },
      "source": [
        "%%markdown\n",
        "## The Input Layer\n",
        "\n",
        "The input layer is the first layer of a neural network. Its primary role is to receive the initial data that the network will process. Each neuron in the input layer typically corresponds to a single feature in the input data.\n",
        "\n",
        "Think of the input layer as the gateway through which your dataset enters the neural network. If your dataset has `n` features, your input layer will typically have `n` neurons, each responsible for receiving the values of one feature for each data sample.\n",
        "\n",
        "### Ensuring Correct Data Dimensions\n",
        "\n",
        "Before feeding your data into the input layer, it's crucial to ensure that it has the correct dimensions. The dimensions of your input data should match the expected input shape of your network.\n",
        "\n",
        "For a typical dataset used in machine learning, the data is organized in a 2D structure (like a spreadsheet or a pandas DataFrame), where:\n",
        "\n",
        "*   The number of rows represents the number of samples or data points.\n",
        "*   The number of columns represents the number of features.\n",
        "\n",
        "The input layer expects data in this format, where each row is a sample and each column is a feature. The number of columns must match the number of neurons in the input layer.\n",
        "\n",
        "Let's check the dimensions of our input data `X`."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## The Input Layer\n\nThe input layer is the first layer of a neural network. Its primary role is to receive the initial data that the network will process. Each neuron in the input layer typically corresponds to a single feature in the input data.\n\nThink of the input layer as the gateway through which your dataset enters the neural network. If your dataset has `n` features, your input layer will typically have `n` neurons, each responsible for receiving the values of one feature for each data sample.\n\n### Ensuring Correct Data Dimensions\n\nBefore feeding your data into the input layer, it's crucial to ensure that it has the correct dimensions. The dimensions of your input data should match the expected input shape of your network.\n\nFor a typical dataset used in machine learning, the data is organized in a 2D structure (like a spreadsheet or a pandas DataFrame), where:\n\n*   The number of rows represents the number of samples or data points.\n*   The number of columns represents the number of features.\n\nThe input layer expects data in this format, where each row is a sample and each column is a feature. The number of columns must match the number of neurons in the input layer.\n\nLet's check the dimensions of our input data `X`.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b55572e"
      },
      "source": [
        "**Reasoning**:\n",
        "Demonstrate how to check the dimensions of the input data `X` using `.shape`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6634dfd",
        "outputId": "55615537-a942-4943-fbc3-242f62e9bbb7"
      },
      "source": [
        "# Check the dimensions of the input data X\n",
        "print(\"Shape of the input data X:\", X.shape)\n",
        "\n",
        "# The shape of X tells us the number of samples (rows) and the number of features (columns).\n",
        "# The number of features (columns) is the dimension that matters for the input layer;\n",
        "# it determines how many neurons the input layer should have.\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the input data X: (100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023e0c57"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "### Discuss various mathematical functions that can be used as activation functions and implement some of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "2200faf7",
        "outputId": "db17ccaf-ee82-4a47-c982-163c9d2bb8ef"
      },
      "source": [
        "%%markdown\n",
        "## Activation Functions: Introducing Non-Linearity\n",
        "\n",
        "In the previous section, we saw that a neuron calculates a weighted sum of its inputs and adds a bias. If we were to simply output this weighted sum, our neural network would only be able to learn linear relationships between the inputs and the output. This is because a sum of linear functions is still a linear function.\n",
        "\n",
        "However, most real-world data involves complex, non-linear relationships. To enable a neural network to learn these complex patterns, we introduce **activation functions**. An activation function takes the output of the weighted sum (often denoted as $Z$) and transforms it into the neuron's final output. This transformation introduces non-linearity, allowing the network to model and learn from non-linear data.\n",
        "\n",
        "Activation functions are applied to the output of each neuron in the hidden layers and often in the output layer.\n",
        "\n",
        "Here are some common types of activation functions:\n",
        "\n",
        "### 1. Sigmoid Function\n",
        "\n",
        "The Sigmoid function, also known as the logistic function, is a classic choice, especially for the output layer in binary classification problems. It squashes the input values between 0 and 1.\n",
        "\n",
        "*   **Mathematical Formula:** $ \\sigma(Z) = \\frac{1}{1 + e^{-Z}} $\n",
        "*   **Description:** Outputs values between 0 and 1, making it useful for representing probabilities. However, it suffers from the \"vanishing gradient\" problem for very large or very small input values, which can slow down training.\n",
        "\n",
        "### 2. Rectified Linear Unit (ReLU)\n",
        "\n",
        "The ReLU function is one of the most popular activation functions in deep learning today. It's computationally efficient and helps mitigate the vanishing gradient problem seen in Sigmoid and Tanh for positive inputs.\n",
        "\n",
        "*   **Mathematical Formula:** $ ReLU(Z) = \\max(0, Z) $\n",
        "*   **Description:** Outputs the input directly if it's positive, otherwise, it outputs zero. It's simple and effective, but can suffer from the \"dying ReLU\" problem where neurons can become inactive for negative inputs.\n",
        "\n",
        "### 3. Hyperbolic Tangent (Tanh)\n",
        "\n",
        "The Tanh function is another common activation function. It is similar to the Sigmoid function but squashes the input values between -1 and 1.\n",
        "\n",
        "*   **Mathematical Formula:** $ tanh(Z) = \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}} $\n",
        "*   **Description:** Outputs values between -1 and 1, which can be beneficial as it centers the output around zero. Like Sigmoid, it can suffer from the vanishing gradient problem.\n",
        "\n",
        "### Other Activation Functions\n",
        "\n",
        "*   **Leaky ReLU:** An improvement over ReLU that addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient for negative inputs.\n",
        "    *   **Mathematical Formula:** $ LeakyReLU(Z) = \\max(\\alpha Z, Z) $ where $\\alpha$ is a small positive constant (e.g., 0.01).\n",
        "*   **Softmax:** Typically used in the output layer of multi-class classification problems. It converts a vector of raw scores into a vector of probabilities that sum up to 1.\n",
        "    *   **Mathematical Formula:** For a vector of inputs $Z = [z_1, z_2, ..., z_k]$, the Softmax of the i-th element is $ Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} $"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Activation Functions: Introducing Non-Linearity\n\nIn the previous section, we saw that a neuron calculates a weighted sum of its inputs and adds a bias. If we were to simply output this weighted sum, our neural network would only be able to learn linear relationships between the inputs and the output. This is because a sum of linear functions is still a linear function.\n\nHowever, most real-world data involves complex, non-linear relationships. To enable a neural network to learn these complex patterns, we introduce **activation functions**. An activation function takes the output of the weighted sum (often denoted as $Z$) and transforms it into the neuron's final output. This transformation introduces non-linearity, allowing the network to model and learn from non-linear data.\n\nActivation functions are applied to the output of each neuron in the hidden layers and often in the output layer.\n\nHere are some common types of activation functions:\n\n### 1. Sigmoid Function\n\nThe Sigmoid function, also known as the logistic function, is a classic choice, especially for the output layer in binary classification problems. It squashes the input values between 0 and 1.\n\n*   **Mathematical Formula:** $ \\sigma(Z) = \\frac{1}{1 + e^{-Z}} $\n*   **Description:** Outputs values between 0 and 1, making it useful for representing probabilities. However, it suffers from the \"vanishing gradient\" problem for very large or very small input values, which can slow down training.\n\n### 2. Rectified Linear Unit (ReLU)\n\nThe ReLU function is one of the most popular activation functions in deep learning today. It's computationally efficient and helps mitigate the vanishing gradient problem seen in Sigmoid and Tanh for positive inputs.\n\n*   **Mathematical Formula:** $ ReLU(Z) = \\max(0, Z) $\n*   **Description:** Outputs the input directly if it's positive, otherwise, it outputs zero. It's simple and effective, but can suffer from the \"dying ReLU\" problem where neurons can become inactive for negative inputs.\n\n### 3. Hyperbolic Tangent (Tanh)\n\nThe Tanh function is another common activation function. It is similar to the Sigmoid function but squashes the input values between -1 and 1.\n\n*   **Mathematical Formula:** $ tanh(Z) = \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}} $\n*   **Description:** Outputs values between -1 and 1, which can be beneficial as it centers the output around zero. Like Sigmoid, it can suffer from the vanishing gradient problem.\n\n### Other Activation Functions\n\n*   **Leaky ReLU:** An improvement over ReLU that addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient for negative inputs.\n    *   **Mathematical Formula:** $ LeakyReLU(Z) = \\max(\\alpha Z, Z) $ where $\\alpha$ is a small positive constant (e.g., 0.01).\n*   **Softmax:** Typically used in the output layer of multi-class classification problems. It converts a vector of raw scores into a vector of probabilities that sum up to 1.\n    *   **Mathematical Formula:** For a vector of inputs $Z = [z_1, z_2, ..., z_k]$, the Softmax of the i-th element is $ Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} $\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8c4ffea"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement Python functions for Sigmoid, ReLU, and Tanh activation functions and demonstrate their usage with example weighted sum values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "135449cc",
        "outputId": "d536e247-0bca-4354-acca-e06865c2b7cc"
      },
      "source": [
        "# Implement the Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Implements the Sigmoid activation function.\n",
        "\n",
        "  Args:\n",
        "    z: The input (weighted sum).\n",
        "\n",
        "  Returns:\n",
        "    The output of the Sigmoid function.\n",
        "  \"\"\"\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Implement the ReLU activation function\n",
        "def relu(z):\n",
        "  \"\"\"\n",
        "  Implements the ReLU activation function.\n",
        "\n",
        "  Args:\n",
        "    z: The input (weighted sum).\n",
        "\n",
        "  Returns:\n",
        "    The output of the ReLU function.\n",
        "  \"\"\"\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "# Implement the Tanh activation function\n",
        "def tanh(z):\n",
        "  \"\"\"\n",
        "  Implements the Tanh activation function.\n",
        "\n",
        "  Args:\n",
        "    z: The input (weighted sum).\n",
        "\n",
        "  Returns:\n",
        "    The output of the Tanh function.\n",
        "  \"\"\"\n",
        "  return np.tanh(z)\n",
        "\n",
        "# Demonstrate the usage of each activation function\n",
        "# Example weighted sum values\n",
        "example_weighted_sums = np.array([-5.0, -1.0, 0.0, 1.0, 5.0])\n",
        "\n",
        "print(f\"Example Weighted Sums: {example_weighted_sums}\")\n",
        "\n",
        "# Apply and print results for each activation function\n",
        "print(\"\\nSigmoid Activation:\")\n",
        "print(sigmoid(example_weighted_sums))\n",
        "\n",
        "print(\"\\nReLU Activation:\")\n",
        "print(relu(example_weighted_sums))\n",
        "\n",
        "print(\"\\nTanh Activation:\")\n",
        "print(tanh(example_weighted_sums))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Weighted Sums: [-5. -1.  0.  1.  5.]\n",
            "\n",
            "Sigmoid Activation:\n",
            "[0.00669285 0.26894142 0.5        0.73105858 0.99330715]\n",
            "\n",
            "ReLU Activation:\n",
            "[0. 0. 0. 1. 5.]\n",
            "\n",
            "Tanh Activation:\n",
            "[-0.9999092  -0.76159416  0.          0.76159416  0.9999092 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0707437d"
      },
      "source": [
        "## Output layer\n",
        "\n",
        "### Explain the output layer, how to select the appropriate activation function for the task, and how to tune thresholds if necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "0f73bf96",
        "outputId": "6521449e-bb96-4786-eeae-0d844e47e15b"
      },
      "source": [
        "%%markdown\n",
        "## The Output Layer: Making Predictions\n",
        "\n",
        "The output layer is the final layer of a neural network. It receives input from the last hidden layer (or the input layer if there are no hidden layers) and produces the network's final output or prediction. The number of neurons in the output layer depends directly on the type of problem the neural network is designed to solve.\n",
        "\n",
        "### Choosing the Output Layer Activation Function\n",
        "\n",
        "The choice of activation function for the output layer is crucial and depends on the nature of the task:\n",
        "\n",
        "*   **Binary Classification (Two Classes):** For problems where the output is one of two classes (e.g., spam or not spam, yes or no), a single neuron with a **Sigmoid** activation function is typically used. The Sigmoid function outputs a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.\n",
        "    *   *Example:* Predicting if an email is spam (1) or not spam (0). The Sigmoid output could be the probability of the email being spam.\n",
        "\n",
        "*   **Multi-Class Classification (More than Two Classes):** When there are more than two mutually exclusive classes (e.g., classifying images of digits 0-9), the output layer will have one neuron for each class, and a **Softmax** activation function is used across all neurons in this layer. Softmax converts a vector of raw scores into a probability distribution, where the sum of probabilities for all classes is 1. The class with the highest probability is the network's prediction.\n",
        "    *   *Example:* Classifying images of handwritten digits into one of 10 classes (0-9). The Softmax output will give the probability for each digit.\n",
        "\n",
        "*   **Regression (Predicting Continuous Values):** For tasks where the goal is to predict a continuous numerical value (e.g., predicting house prices, stock prices), the output layer typically has a single neuron with a **linear** or no activation function. A linear activation simply outputs the weighted sum directly.\n",
        "    *   *Example:* Predicting the price of a house based on its features. The output neuron directly outputs the predicted price.\n",
        "\n",
        "### Tuning the Threshold for Binary Classification\n",
        "\n",
        "In binary classification problems using a Sigmoid output, the output value represents a probability. To convert this probability into a binary class prediction (0 or 1), a **threshold** is applied.\n",
        "\n",
        "*   **Standard Threshold:** A common threshold is 0.5. If the Sigmoid output is greater than or equal to 0.5, the prediction is class 1; otherwise, it's class 0.\n",
        "*   **Tuning the Threshold:** The choice of threshold can impact the trade-off between precision and recall. Depending on the specific problem and the costs associated with false positives and false negatives, you might want to adjust this threshold.\n",
        "    *   *Lowering the threshold* (e.g., to 0.4) makes it easier for an instance to be classified as class 1, increasing recall but potentially decreasing precision.\n",
        "    *   *Raising the threshold* (e.g., to 0.6) makes it harder for an instance to be classified as class 1, increasing precision but potentially decreasing recall.\n",
        "\n",
        "Tuning the threshold is often done after the model has been trained and involves evaluating the model's performance at different threshold values on a validation set.\n",
        "\n",
        "Let's demonstrate applying a Sigmoid activation and a threshold to a hypothetical output."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## The Output Layer: Making Predictions\n\nThe output layer is the final layer of a neural network. It receives input from the last hidden layer (or the input layer if there are no hidden layers) and produces the network's final output or prediction. The number of neurons in the output layer depends directly on the type of problem the neural network is designed to solve.\n\n### Choosing the Output Layer Activation Function\n\nThe choice of activation function for the output layer is crucial and depends on the nature of the task:\n\n*   **Binary Classification (Two Classes):** For problems where the output is one of two classes (e.g., spam or not spam, yes or no), a single neuron with a **Sigmoid** activation function is typically used. The Sigmoid function outputs a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.\n    *   *Example:* Predicting if an email is spam (1) or not spam (0). The Sigmoid output could be the probability of the email being spam.\n\n*   **Multi-Class Classification (More than Two Classes):** When there are more than two mutually exclusive classes (e.g., classifying images of digits 0-9), the output layer will have one neuron for each class, and a **Softmax** activation function is used across all neurons in this layer. Softmax converts a vector of raw scores into a probability distribution, where the sum of probabilities for all classes is 1. The class with the highest probability is the network's prediction.\n    *   *Example:* Classifying images of handwritten digits into one of 10 classes (0-9). The Softmax output will give the probability for each digit.\n\n*   **Regression (Predicting Continuous Values):** For tasks where the goal is to predict a continuous numerical value (e.g., predicting house prices, stock prices), the output layer typically has a single neuron with a **linear** or no activation function. A linear activation simply outputs the weighted sum directly.\n    *   *Example:* Predicting the price of a house based on its features. The output neuron directly outputs the predicted price.\n\n### Tuning the Threshold for Binary Classification\n\nIn binary classification problems using a Sigmoid output, the output value represents a probability. To convert this probability into a binary class prediction (0 or 1), a **threshold** is applied.\n\n*   **Standard Threshold:** A common threshold is 0.5. If the Sigmoid output is greater than or equal to 0.5, the prediction is class 1; otherwise, it's class 0.\n*   **Tuning the Threshold:** The choice of threshold can impact the trade-off between precision and recall. Depending on the specific problem and the costs associated with false positives and false negatives, you might want to adjust this threshold.\n    *   *Lowering the threshold* (e.g., to 0.4) makes it easier for an instance to be classified as class 1, increasing recall but potentially decreasing precision.\n    *   *Raising the threshold* (e.g., to 0.6) makes it harder for an instance to be classified as class 1, increasing precision but potentially decreasing recall.\n\nTuning the threshold is often done after the model has been trained and involves evaluating the model's performance at different threshold values on a validation set.\n\nLet's demonstrate applying a Sigmoid activation and a threshold to a hypothetical output.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee79cdd"
      },
      "source": [
        "**Reasoning**:\n",
        "Demonstrate applying a Sigmoid activation function to a hypothetical output and then applying a threshold to make a binary prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "896fb2a7",
        "outputId": "beebcab9-7d5a-4e87-bdba-35b8360fa6c5"
      },
      "source": [
        "# Assume a hypothetical weighted sum output from the last layer neuron\n",
        "hypothetical_weighted_sum = 1.5\n",
        "\n",
        "# Apply the Sigmoid activation function (using the function defined previously)\n",
        "sigmoid_output = sigmoid(hypothetical_weighted_sum)\n",
        "\n",
        "print(f\"Hypothetical Weighted Sum: {hypothetical_weighted_sum}\")\n",
        "print(f\"Output after Sigmoid Activation: {sigmoid_output:.4f}\")\n",
        "\n",
        "# Define a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Apply the threshold to make a binary prediction\n",
        "binary_prediction = 1 if sigmoid_output >= threshold else 0\n",
        "\n",
        "print(f\"Threshold: {threshold}\")\n",
        "print(f\"Binary Prediction: {binary_prediction}\")\n",
        "\n",
        "# Demonstrate with another hypothetical weighted sum\n",
        "hypothetical_weighted_sum_2 = -0.5\n",
        "sigmoid_output_2 = sigmoid(hypothetical_weighted_sum_2)\n",
        "\n",
        "print(f\"\\nAnother Hypothetical Weighted Sum: {hypothetical_weighted_sum_2}\")\n",
        "print(f\"Output after Sigmoid Activation: {sigmoid_output_2:.4f}\")\n",
        "\n",
        "binary_prediction_2 = 1 if sigmoid_output_2 >= threshold else 0\n",
        "print(f\"Threshold: {threshold}\")\n",
        "print(f\"Binary Prediction: {binary_prediction_2}\")\n",
        "\n",
        "# Demonstrate with a different threshold\n",
        "alternative_threshold = 0.6\n",
        "binary_prediction_3 = 1 if sigmoid_output >= alternative_threshold else 0\n",
        "\n",
        "print(f\"\\nOriginal Sigmoid Output: {sigmoid_output:.4f}\")\n",
        "print(f\"Alternative Threshold: {alternative_threshold}\")\n",
        "print(f\"Binary Prediction with Alternative Threshold: {binary_prediction_3}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypothetical Weighted Sum: 1.5\n",
            "Output after Sigmoid Activation: 0.8176\n",
            "Threshold: 0.5\n",
            "Binary Prediction: 1\n",
            "\n",
            "Another Hypothetical Weighted Sum: -0.5\n",
            "Output after Sigmoid Activation: 0.3775\n",
            "Threshold: 0.5\n",
            "Binary Prediction: 0\n",
            "\n",
            "Original Sigmoid Output: 0.8176\n",
            "Alternative Threshold: 0.6\n",
            "Binary Prediction with Alternative Threshold: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8607889a"
      },
      "source": [
        "## Hidden layer\n",
        "\n",
        "### Explain the purpose of hidden layers and how to code neurons within them, selecting appropriate activation functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "f6df48e8",
        "outputId": "2472ba6f-c6f6-42f3-a3b4-cbfc9f1a0b80"
      },
      "source": [
        "%%markdown\n",
        "## Hidden Layers: Learning Complex Patterns\n",
        "\n",
        "Between the input layer and the output layer are one or more **hidden layers**. These layers are where the majority of the computation in a neural network takes place. They are called \"hidden\" because their inputs and outputs are not directly exposed to the outside world; they are internal to the network.\n",
        "\n",
        "### The Role of Hidden Layers\n",
        "\n",
        "The primary purpose of hidden layers is to learn increasingly complex and abstract representations of the input data. Each neuron in a hidden layer receives inputs from the previous layer (either the input layer or a preceding hidden layer) and transforms these inputs using weights, biases, and an activation function.\n",
        "\n",
        "As data passes through successive hidden layers, the network can identify more intricate patterns and features. For example, in an image recognition task, the first hidden layer might learn to detect edges, the next layer might combine edges to detect shapes, and subsequent layers might combine shapes to recognize objects.\n",
        "\n",
        "### Why Non-Linear Activation Functions are Crucial\n",
        "\n",
        "We discussed the importance of activation functions in introducing non-linearity within a single neuron. This non-linearity is absolutely essential in hidden layers.\n",
        "\n",
        "*   **Without non-linear activation functions in hidden layers**, a deep neural network (with multiple layers) would essentially behave like a single-layer linear model, no matter how many layers it has. This is because a composition of linear functions is always a linear function.\n",
        "*   **With non-linear activation functions**, hidden layers can learn non-linear relationships and complex interactions between features. This allows neural networks to model highly intricate patterns in the data that linear models cannot capture.\n",
        "\n",
        "Common non-linear activation functions used in hidden layers include ReLU, Tanh, and their variants.\n",
        "\n",
        "Let's see how to calculate the output of a neuron in a hidden layer, incorporating the concepts of weighted sum and a non-linear activation function."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Hidden Layers: Learning Complex Patterns\n\nBetween the input layer and the output layer are one or more **hidden layers**. These layers are where the majority of the computation in a neural network takes place. They are called \"hidden\" because their inputs and outputs are not directly exposed to the outside world; they are internal to the network.\n\n### The Role of Hidden Layers\n\nThe primary purpose of hidden layers is to learn increasingly complex and abstract representations of the input data. Each neuron in a hidden layer receives inputs from the previous layer (either the input layer or a preceding hidden layer) and transforms these inputs using weights, biases, and an activation function.\n\nAs data passes through successive hidden layers, the network can identify more intricate patterns and features. For example, in an image recognition task, the first hidden layer might learn to detect edges, the next layer might combine edges to detect shapes, and subsequent layers might combine shapes to recognize objects.\n\n### Why Non-Linear Activation Functions are Crucial\n\nWe discussed the importance of activation functions in introducing non-linearity within a single neuron. This non-linearity is absolutely essential in hidden layers.\n\n*   **Without non-linear activation functions in hidden layers**, a deep neural network (with multiple layers) would essentially behave like a single-layer linear model, no matter how many layers it has. This is because a composition of linear functions is always a linear function.\n*   **With non-linear activation functions**, hidden layers can learn non-linear relationships and complex interactions between features. This allows neural networks to model highly intricate patterns in the data that linear models cannot capture.\n\nCommon non-linear activation functions used in hidden layers include ReLU, Tanh, and their variants.\n\nLet's see how to calculate the output of a neuron in a hidden layer, incorporating the concepts of weighted sum and a non-linear activation function.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5abcc12"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide a simple Python code example to demonstrate calculating the output of a neuron in a hidden layer, including taking inputs, using random weights and bias, calculating the weighted sum, and applying a non-linear activation function (ReLU).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea506d1a",
        "outputId": "b1ac85bb-1786-46c9-87f4-ef82024861ef"
      },
      "source": [
        "# Demonstrate calculating the output of a hidden layer neuron\n",
        "\n",
        "# For this example, let's use the first data sample from X as the input\n",
        "# In a real network, these inputs would be the outputs of the previous layer\n",
        "hidden_neuron_inputs = np.array([X['feature1'][0], X['feature2'][0]])\n",
        "\n",
        "# Randomly initialize weights and bias for a single hidden neuron\n",
        "# The number of weights must match the number of inputs\n",
        "hidden_neuron_weights = np.random.rand(len(hidden_neuron_inputs)) * 0.5 - 0.25 # Small random values around 0\n",
        "hidden_neuron_bias = np.random.rand() * 0.5 - 0.25 # Small random value around 0\n",
        "\n",
        "print(f\"Inputs to the Hidden Neuron: {hidden_neuron_inputs}\")\n",
        "print(f\"Weights for the Hidden Neuron: {hidden_neuron_weights}\")\n",
        "print(f\"Bias for the Hidden Neuron: {hidden_neuron_bias}\")\n",
        "\n",
        "# Calculate the weighted sum for the hidden neuron (using the function defined previously)\n",
        "hidden_neuron_weighted_sum = weighted_sum(hidden_neuron_inputs, hidden_neuron_weights, hidden_neuron_bias)\n",
        "print(f\"Weighted Sum (Z) for Hidden Neuron: {hidden_neuron_weighted_sum}\")\n",
        "\n",
        "# Apply a non-linear activation function (e.g., ReLU) to the weighted sum\n",
        "# We will use the relu function defined previously\n",
        "hidden_neuron_output = relu(hidden_neuron_weighted_sum)\n",
        "print(f\"Output after ReLU Activation: {hidden_neuron_output}\")\n",
        "\n",
        "# Demonstrate with another activation function (e.g., Tanh)\n",
        "hidden_neuron_output_tanh = tanh(hidden_neuron_weighted_sum)\n",
        "print(f\"Output after Tanh Activation: {hidden_neuron_output_tanh}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs to the Hidden Neuron: [3.74540119 2.04679473]\n",
            "Weights for the Hidden Neuron: [ 0.00815017 -0.08852176]\n",
            "Bias for the Hidden Neuron: 0.14759309738435183\n",
            "Weighted Sum (Z) for Hidden Neuron: -0.003067109811097507\n",
            "Output after ReLU Activation: 0.0\n",
            "Output after Tanh Activation: -0.0030671001935334685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4742d2f1"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "### Create Python functions for the forward pass of the neural network, initialize weights and biases, and demonstrate the forward pass with the input data X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "422b4d04",
        "outputId": "d728e3b4-1216-45eb-b775-56bcfeb7c330"
      },
      "source": [
        "%%markdown\n",
        "## Building the Neural Network: Combining the Layers\n",
        "\n",
        "Now that we understand the fundamental components – the neuron, input layer, activation functions, hidden layers, and output layer – we can combine them to build a simple neural network.\n",
        "\n",
        "For this demonstration, we will create a network with:\n",
        "*   An input layer (with a number of neurons equal to the number of features in `X`)\n",
        "*   One hidden layer with 2 neurons\n",
        "*   An output layer with 1 neuron (for binary classification)\n",
        "\n",
        "We will implement the **forward pass**, which is the process of feeding the input data through the network, layer by layer, to produce an output prediction.\n",
        "\n",
        "The forward pass involves:\n",
        "1.  Taking the input data.\n",
        "2.  Calculating the weighted sum and applying an activation function for each neuron in the hidden layer.\n",
        "3.  Using the outputs of the hidden layer as inputs to the output layer.\n",
        "4.  Calculating the weighted sum and applying the output layer's activation function (Sigmoid in our case) for the output neuron.\n",
        "5.  The final output is the network's prediction.\n",
        "\n",
        "Let's define Python functions for the forward pass and then initialize random weights and biases for our network."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Building the Neural Network: Combining the Layers\n\nNow that we understand the fundamental components – the neuron, input layer, activation functions, hidden layers, and output layer – we can combine them to build a simple neural network.\n\nFor this demonstration, we will create a network with:\n*   An input layer (with a number of neurons equal to the number of features in `X`)\n*   One hidden layer with 2 neurons\n*   An output layer with 1 neuron (for binary classification)\n\nWe will implement the **forward pass**, which is the process of feeding the input data through the network, layer by layer, to produce an output prediction.\n\nThe forward pass involves:\n1.  Taking the input data.\n2.  Calculating the weighted sum and applying an activation function for each neuron in the hidden layer.\n3.  Using the outputs of the hidden layer as inputs to the output layer.\n4.  Calculating the weighted sum and applying the output layer's activation function (Sigmoid in our case) for the output neuron.\n5.  The final output is the network's prediction.\n\nLet's define Python functions for the forward pass and then initialize random weights and biases for our network.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08a1a069"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the forward pass function, initialize weights and biases for the network layers, and demonstrate the forward pass using the input data X.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bc11a40",
        "outputId": "17d7bf41-f796-4798-d3a0-0aa23d8aaa1d"
      },
      "source": [
        "# 1. Create Python functions to represent the forward pass\n",
        "def forward_pass(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
        "  \"\"\"\n",
        "  Represents the forward pass through a simple neural network.\n",
        "\n",
        "  Args:\n",
        "    X: Input data (numpy array or pandas DataFrame).\n",
        "    weights_hidden: Weights for the hidden layer.\n",
        "    bias_hidden: Bias for the hidden layer.\n",
        "    weights_output: Weights for the output layer.\n",
        "    bias_output: Bias for the output layer.\n",
        "\n",
        "  Returns:\n",
        "    The output of the neural network's output layer activation.\n",
        "  \"\"\"\n",
        "  # Ensure X is a numpy array for matrix multiplication\n",
        "  X = np.array(X)\n",
        "\n",
        "  # Calculate the weighted sum for the hidden layer\n",
        "  hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden\n",
        "\n",
        "  # Apply the activation function for the hidden layer (using ReLU)\n",
        "  hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "  # Calculate the weighted sum for the output layer\n",
        "  output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
        "\n",
        "  # Apply the activation function for the output layer (using Sigmoid)\n",
        "  output_layer_output = sigmoid(output_layer_input)\n",
        "\n",
        "  return output_layer_output\n",
        "\n",
        "# 6. Initialize random weights and biases\n",
        "# Number of input features is the number of columns in X\n",
        "n_input_features = X.shape[1]\n",
        "# Desired number of neurons in the hidden layer\n",
        "n_hidden_neurons = 2\n",
        "# Desired number of neurons in the output layer\n",
        "n_output_neurons = 1\n",
        "\n",
        "# Initialize weights and biases for the hidden layer\n",
        "# Weights shape: (number of input features, number of hidden neurons)\n",
        "weights_hidden = np.random.rand(n_input_features, n_hidden_neurons) * 0.1 # Small random values\n",
        "bias_hidden = np.random.rand(n_hidden_neurons) * 0.1 # Small random values\n",
        "\n",
        "# Initialize weights and biases for the output layer\n",
        "# Weights shape: (number of hidden neurons, number of output neurons)\n",
        "weights_output = np.random.rand(n_hidden_neurons, n_output_neurons) * 0.1 # Small random values\n",
        "bias_output = np.random.rand(n_output_neurons) * 0.1 # Small random values\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of weights_hidden:\", weights_hidden.shape)\n",
        "print(\"Shape of bias_hidden:\", bias_hidden.shape)\n",
        "print(\"Shape of weights_output:\", weights_output.shape)\n",
        "print(\"Shape of bias_output:\", bias_output.shape)\n",
        "\n",
        "\n",
        "# 7. Demonstrate the forward pass\n",
        "network_output = forward_pass(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
        "\n",
        "print(\"\\nOutput of the neural network for the first 5 samples:\")\n",
        "print(network_output[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (100, 2)\n",
            "Shape of weights_hidden: (2, 2)\n",
            "Shape of bias_hidden: (2,)\n",
            "Shape of weights_output: (2, 1)\n",
            "Shape of bias_output: (1,)\n",
            "\n",
            "Output of the neural network for the first 5 samples:\n",
            "[[0.51063872]\n",
            " [0.51627805]\n",
            " [0.51421699]\n",
            " [0.51220791]\n",
            " [0.50835198]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868892c2"
      },
      "source": [
        "## Training the Neural Network\n",
        "\n",
        "### Explain and implement a basic training process of training a neural network, including loss functions and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "30da5477",
        "outputId": "6a25b661-2447-483a-843d-0755c8e7c0f6"
      },
      "source": [
        "%%markdown\n",
        "## Training the Neural Network: Learning from Data\n",
        "\n",
        "So far, we've built the structure of a simple neural network and implemented the **forward pass**, which allows us to make predictions based on given inputs and the network's current weights and biases. However, with randomly initialized weights and biases, our network's predictions are likely to be inaccurate.\n",
        "\n",
        "The goal of training a neural network is to adjust these weights and biases so that the network's predictions are as close as possible to the actual target values in our dataset. This learning process is typically achieved using an **optimization algorithm**.\n",
        "\n",
        "### Loss Function: Measuring Prediction Error\n",
        "\n",
        "Before we can improve our predictions, we need a way to measure how \"wrong\" they are. This is the role of the **loss function** (also known as the cost function or error function). The loss function quantifies the difference between the network's predicted output and the true target values. A higher loss indicates poorer performance, while a lower loss indicates better performance.\n",
        "\n",
        "The choice of loss function depends on the type of problem:\n",
        "\n",
        "*   **Binary Classification:** **Binary Cross-Entropy (BCE)** is a common choice. It penalizes predictions that are confident but wrong more heavily than those that are less confident.\n",
        "*   **Multi-Class Classification:** **Categorical Cross-Entropy** is typically used.\n",
        "*   **Regression:** **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)** are often used.\n",
        "\n",
        "Our current problem is binary classification, so we will use the Binary Cross-Entropy loss function.\n",
        "\n",
        "### Optimization Algorithm: Minimizing the Loss\n",
        "\n",
        "Once we can measure the error using a loss function, we need a method to systematically adjust the weights and biases to minimize this error. This is where **optimization algorithms** come in.\n",
        "\n",
        "One of the most fundamental and widely used optimization algorithms is **Gradient Descent**.\n",
        "\n",
        "#### Gradient Descent\n",
        "\n",
        "Imagine the loss function as a landscape with hills and valleys, where the \"height\" represents the loss and the \"position\" represents the values of the network's weights and biases. The goal is to find the lowest point in this landscape, which corresponds to the set of weights and biases that minimizes the loss.\n",
        "\n",
        "Gradient Descent works by iteratively taking steps in the direction of the **steepest descent** in the loss landscape. The direction of the steepest descent is given by the negative of the **gradient** of the loss function with respect to each weight and bias.\n",
        "\n",
        "*   **Gradient:** The gradient is a vector of partial derivatives. For each weight and each bias in the network, the gradient tells us how much the loss function changes when that specific weight or bias is slightly changed.\n",
        "*   **Learning Rate:** The size of the steps taken in the direction of the negative gradient is controlled by a parameter called the **learning rate**. A larger learning rate means bigger steps, which can lead to faster convergence but might overshoot the minimum. A smaller learning rate means smaller steps, which can lead to more stable convergence but might be slow.\n",
        "\n",
        "The process involves:\n",
        "1.  **Forward Pass:** Calculate the network's output and the loss for the current inputs.\n",
        "2.  **Backward Pass (Backpropagation):** Calculate the gradient of the loss with respect to each weight and bias in the network, working backward from the output layer to the input layer using the chain rule of calculus.\n",
        "3.  **Parameter Update:** Adjust the weights and biases by subtracting a fraction of their gradients (determined by the learning rate).\n",
        "\n",
        "This iterative process of forward pass, backward pass, and parameter update is repeated for many **epochs** (one full pass through the entire training dataset) until the network's performance is satisfactory or the loss converges.\n",
        "\n",
        "Let's implement the Binary Cross-Entropy loss function and then delve into the backward pass and gradient descent."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Training the Neural Network: Learning from Data\n\nSo far, we've built the structure of a simple neural network and implemented the **forward pass**, which allows us to make predictions based on given inputs and the network's current weights and biases. However, with randomly initialized weights and biases, our network's predictions are likely to be inaccurate.\n\nThe goal of training a neural network is to adjust these weights and biases so that the network's predictions are as close as possible to the actual target values in our dataset. This learning process is typically achieved using an **optimization algorithm**.\n\n### Loss Function: Measuring Prediction Error\n\nBefore we can improve our predictions, we need a way to measure how \"wrong\" they are. This is the role of the **loss function** (also known as the cost function or error function). The loss function quantifies the difference between the network's predicted output and the true target values. A higher loss indicates poorer performance, while a lower loss indicates better performance.\n\nThe choice of loss function depends on the type of problem:\n\n*   **Binary Classification:** **Binary Cross-Entropy (BCE)** is a common choice. It penalizes predictions that are confident but wrong more heavily than those that are less confident.\n*   **Multi-Class Classification:** **Categorical Cross-Entropy** is typically used.\n*   **Regression:** **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)** are often used.\n\nOur current problem is binary classification, so we will use the Binary Cross-Entropy loss function.\n\n### Optimization Algorithm: Minimizing the Loss\n\nOnce we can measure the error using a loss function, we need a method to systematically adjust the weights and biases to minimize this error. This is where **optimization algorithms** come in.\n\nOne of the most fundamental and widely used optimization algorithms is **Gradient Descent**.\n\n#### Gradient Descent\n\nImagine the loss function as a landscape with hills and valleys, where the \"height\" represents the loss and the \"position\" represents the values of the network's weights and biases. The goal is to find the lowest point in this landscape, which corresponds to the set of weights and biases that minimizes the loss.\n\nGradient Descent works by iteratively taking steps in the direction of the **steepest descent** in the loss landscape. The direction of the steepest descent is given by the negative of the **gradient** of the loss function with respect to each weight and bias.\n\n*   **Gradient:** The gradient is a vector of partial derivatives. For each weight and each bias in the network, the gradient tells us how much the loss function changes when that specific weight or bias is slightly changed.\n*   **Learning Rate:** The size of the steps taken in the direction of the negative gradient is controlled by a parameter called the **learning rate**. A larger learning rate means bigger steps, which can lead to faster convergence but might overshoot the minimum. A smaller learning rate means smaller steps, which can lead to more stable convergence but might be slow.\n\nThe process involves:\n1.  **Forward Pass:** Calculate the network's output and the loss for the current inputs.\n2.  **Backward Pass (Backpropagation):** Calculate the gradient of the loss with respect to each weight and bias in the network, working backward from the output layer to the input layer using the chain rule of calculus.\n3.  **Parameter Update:** Adjust the weights and biases by subtracting a fraction of their gradients (determined by the learning rate).\n\nThis iterative process of forward pass, backward pass, and parameter update is repeated for many **epochs** (one full pass through the entire training dataset) until the network's performance is satisfactory or the loss converges.\n\nLet's implement the Binary Cross-Entropy loss function and then delve into the backward pass and gradient descent.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ac42d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the Binary Cross-Entropy loss function as described in the markdown cell and instruction 2 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bc9cf55",
        "outputId": "7c766fd4-cd83-4d8e-cef4-4c5cbfa0fa8a"
      },
      "source": [
        "# 2. Implement a loss function suitable for binary classification (Binary Cross-Entropy)\n",
        "def binary_cross_entropy_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates the Binary Cross-Entropy loss.\n",
        "\n",
        "  Args:\n",
        "    y_true: The true binary labels (0 or 1).\n",
        "    y_pred: The predicted probabilities (output of the Sigmoid function).\n",
        "\n",
        "  Returns:\n",
        "    The average Binary Cross-Entropy loss.\n",
        "  \"\"\"\n",
        "  # Avoid log(0) by clipping predicted values\n",
        "  epsilon = 1e-15\n",
        "  y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "  # Binary Cross-Entropy formula: - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
        "  loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "  return loss\n",
        "\n",
        "# Demonstrate the usage with example true labels and predicted probabilities\n",
        "example_y_true = np.array([0, 1, 1, 0, 1])\n",
        "example_y_pred = np.array([0.1, 0.9, 0.6, 0.4, 0.8]) # Hypothetical outputs from Sigmoid\n",
        "\n",
        "loss = binary_cross_entropy_loss(example_y_true, example_y_pred)\n",
        "print(f\"Example True Labels: {example_y_true}\")\n",
        "print(f\"Example Predicted Probabilities: {example_y_pred}\")\n",
        "print(f\"Binary Cross-Entropy Loss: {loss:.4f}\")\n",
        "\n",
        "# Demonstrate with predictions that are closer to true values\n",
        "example_y_pred_better = np.array([0.05, 0.95, 0.7, 0.3, 0.9])\n",
        "loss_better = binary_cross_entropy_loss(example_y_true, example_y_pred_better)\n",
        "print(f\"\\nExample Predicted Probabilities (Better): {example_y_pred_better}\")\n",
        "print(f\"Binary Cross-Entropy Loss (Better): {loss_better:.4f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example True Labels: [0 1 1 0 1]\n",
            "Example Predicted Probabilities: [0.1 0.9 0.6 0.4 0.8]\n",
            "Binary Cross-Entropy Loss: 0.2911\n",
            "\n",
            "Example Predicted Probabilities (Better): [0.05 0.95 0.7  0.3  0.9 ]\n",
            "Binary Cross-Entropy Loss (Better): 0.1843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaaebd60"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain and implement the backward pass (backpropagation) and the gradient descent update rule, covering instructions 3 and 4 of the subtask. This requires calculating gradients for both the output and hidden layer parameters and then updating those parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7c761b59",
        "outputId": "b5bfe19e-0d3f-42ed-c065-4600949f9ae6"
      },
      "source": [
        "%%markdown\n",
        "### Backward Pass (Backpropagation) and Gradient Descent\n",
        "\n",
        "Now that we have a way to measure the error (Loss Function), we need to figure out how to adjust the weights and biases to reduce this error. This is done using the **Backward Pass**, also known as **Backpropagation**, and the **Gradient Descent** update rule.\n",
        "\n",
        "#### The Chain Rule: The Engine of Backpropagation\n",
        "\n",
        "Backpropagation uses the **chain rule** of calculus to calculate the gradient of the loss function with respect to each weight and bias in the network. The chain rule allows us to calculate how a change in a weight or bias in an earlier layer affects the final loss, by multiplying the rates of change (derivatives) through each layer.\n",
        "\n",
        "Essentially, we start at the output layer and calculate the gradient of the loss with respect to the output. Then, we propagate this gradient backward through the network, layer by layer, calculating the gradient with respect to the weights and biases in each layer.\n",
        "\n",
        "#### Gradient Calculation Steps (for our simple network):\n",
        "\n",
        "Let $L$ be the Binary Cross-Entropy Loss, $y_{true}$ be the true labels, $y_{pred}$ be the network's output (after Sigmoid), $Z_{output}$ be the weighted sum for the output layer, $A_{hidden}$ be the output of the hidden layer (after ReLU), $Z_{hidden}$ be the weighted sum for the hidden layer, $W_{output}$ and $B_{output}$ be the weights and bias for the output layer, and $W_{hidden}$ and $B_{hidden}$ be the weights and bias for the hidden layer. $X$ is the input data.\n",
        "\n",
        "1.  **Gradient of Loss with respect to Output ($y_{pred}$):**\n",
        "    The derivative of the BCE loss with respect to the Sigmoid output $y_{pred}$ is:\n",
        "    $ \\frac{\\partial L}{\\partial y_{pred}} = -\\left(\\frac{y_{true}}{y_{pred}} - \\frac{1 - y_{true}}{1 - y_{pred}}\\right) $\n",
        "\n",
        "2.  **Gradient of Output ($y_{pred}$) with respect to Output Weighted Sum ($Z_{output}$):**\n",
        "    The derivative of the Sigmoid function is $ \\sigma'(Z) = \\sigma(Z)(1 - \\sigma(Z)) $. So,\n",
        "    $ \\frac{\\partial y_{pred}}{\\partial Z_{output}} = y_{pred} (1 - y_{pred}) $\n",
        "\n",
        "3.  **Gradient of Loss with respect to Output Weighted Sum ($Z_{output}$):**\n",
        "    Using the chain rule:\n",
        "    $ \\frac{\\partial L}{\\partial Z_{output}} = \\frac{\\partial L}{\\partial y_{pred}} \\times \\frac{\\partial y_{pred}}{\\partial Z_{output}} = -\\left(\\frac{y_{true}}{y_{pred}} - \\frac{1 - y_{true}}{1 - y_{pred}}\\right) \\times y_{pred} (1 - y_{pred}) $\n",
        "    This simplifies nicely for BCE and Sigmoid:\n",
        "    $ \\frac{\\partial L}{\\partial Z_{output}} = y_{pred} - y_{true} $\n",
        "\n",
        "4.  **Gradient of Loss with respect to Output Layer Weights ($W_{output}$) and Bias ($B_{output}$):**\n",
        "    Now we can calculate the gradients for the output layer parameters using the chain rule:\n",
        "    $ \\frac{\\partial L}{\\partial W_{output}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial W_{output}} $\n",
        "    Since $Z_{output} = A_{hidden} \\cdot W_{output} + B_{output}$, the derivative of $Z_{output}$ with respect to $W_{output}$ is $A_{hidden}^T$.\n",
        "    $ \\frac{\\partial L}{\\partial W_{output}} = A_{hidden}^T \\cdot (y_{pred} - y_{true}) $ (Note: This is a matrix multiplication)\n",
        "\n",
        "    $ \\frac{\\partial L}{\\partial B_{output}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial B_{output}} $\n",
        "    The derivative of $Z_{output}$ with respect to $B_{output}$ is 1.\n",
        "    $ \\frac{\\partial L}{\\partial B_{output}} = y_{pred} - y_{true} $ (Summed over samples for the bias gradient)\n",
        "\n",
        "5.  **Gradient of Loss with respect to Hidden Layer Output ($A_{hidden}$):**\n",
        "    To propagate the gradient backward to the hidden layer, we need the gradient of the loss with respect to the hidden layer's output:\n",
        "    $ \\frac{\\partial L}{\\partial A_{hidden}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial A_{hidden}} $\n",
        "    Since $Z_{output} = A_{hidden} \\cdot W_{output} + B_{output}$, the derivative of $Z_{output}$ with respect to $A_{hidden}$ is $W_{output}^T$.\n",
        "    $ \\frac{\\partial L}{\\partial A_{hidden}} = (y_{pred} - y_{true}) \\cdot W_{output}^T $ (Note: This is a matrix multiplication)\n",
        "\n",
        "6.  **Gradient of Hidden Layer Output ($A_{hidden}$) with respect to Hidden Layer Weighted Sum ($Z_{hidden}$):**\n",
        "    We used the ReLU activation function for the hidden layer. The derivative of the ReLU function is:\n",
        "    $ ReLU'(Z) = 1 $ if $ Z > 0 $\n",
        "    $ ReLU'(Z) = 0 $ if $ Z \\le 0 $\n",
        "    So, $ \\frac{\\partial A_{hidden}}{\\partial Z_{hidden}} = ReLU'(Z_{hidden}) $ (Element-wise multiplication)\n",
        "\n",
        "7.  **Gradient of Loss with respect to Hidden Layer Weighted Sum ($Z_{hidden}$):**\n",
        "    Using the chain rule:\n",
        "    $ \\frac{\\partial L}{\\partial Z_{hidden}} = \\frac{\\partial L}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial Z_{hidden}} = \\left((y_{pred} - y_{true}) \\cdot W_{output}^T\\right) \\times ReLU'(Z_{hidden}) $ (Element-wise multiplication with the derivative of ReLU)\n",
        "\n",
        "8.  **Gradient of Loss with respect to Hidden Layer Weights ($W_{hidden}$) and Bias ($B_{hidden}$):**\n",
        "    Finally, we can calculate the gradients for the hidden layer parameters:\n",
        "    $ \\frac{\\partial L}{\\partial W_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} \\times \\frac{\\partial Z_{hidden}}{\\partial W_{hidden}} $\n",
        "    Since $Z_{hidden} = X \\cdot W_{hidden} + B_{hidden}$, the derivative of $Z_{hidden}$ with respect to $W_{hidden}$ is $X^T$.\n",
        "    $ \\frac{\\partial L}{\\partial W_{hidden}} = X^T \\cdot \\left(\\frac{\\partial L}{\\partial Z_{hidden}}\\right) $ (Note: This is a matrix multiplication)\n",
        "\n",
        "    $ \\frac{\\partial L}{\\partial B_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} \\times \\frac{\\partial Z_{hidden}}{\\partial B_{hidden}} $\n",
        "    The derivative of $Z_{hidden}$ with respect to $B_{hidden}$ is 1.\n",
        "    $ \\frac{\\partial L}{\\partial B_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} $ (Summed over samples for the bias gradient)\n",
        "\n",
        "#### Gradient Descent Update Rule\n",
        "\n",
        "Once we have the gradients for all weights and biases, we update them using the gradient descent rule:\n",
        "\n",
        "$ Parameter = Parameter - LearningRate \\times Gradient $\n",
        "\n",
        "Where `Parameter` is a weight or bias, `LearningRate` is a small positive value that controls the step size, and `Gradient` is the calculated gradient of the loss with respect to that parameter.\n",
        "\n",
        "Let's implement these gradient calculations and the update rule. We'll need to modify our forward pass function slightly to store the intermediate values ($Z_{hidden}, A_{hidden}, Z_{output}$) which are needed for the backward pass."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Backward Pass (Backpropagation) and Gradient Descent\n\nNow that we have a way to measure the error (Loss Function), we need to figure out how to adjust the weights and biases to reduce this error. This is done using the **Backward Pass**, also known as **Backpropagation**, and the **Gradient Descent** update rule.\n\n#### The Chain Rule: The Engine of Backpropagation\n\nBackpropagation uses the **chain rule** of calculus to calculate the gradient of the loss function with respect to each weight and bias in the network. The chain rule allows us to calculate how a change in a weight or bias in an earlier layer affects the final loss, by multiplying the rates of change (derivatives) through each layer.\n\nEssentially, we start at the output layer and calculate the gradient of the loss with respect to the output. Then, we propagate this gradient backward through the network, layer by layer, calculating the gradient with respect to the weights and biases in each layer.\n\n#### Gradient Calculation Steps (for our simple network):\n\nLet $L$ be the Binary Cross-Entropy Loss, $y_{true}$ be the true labels, $y_{pred}$ be the network's output (after Sigmoid), $Z_{output}$ be the weighted sum for the output layer, $A_{hidden}$ be the output of the hidden layer (after ReLU), $Z_{hidden}$ be the weighted sum for the hidden layer, $W_{output}$ and $B_{output}$ be the weights and bias for the output layer, and $W_{hidden}$ and $B_{hidden}$ be the weights and bias for the hidden layer. $X$ is the input data.\n\n1.  **Gradient of Loss with respect to Output ($y_{pred}$):**\n    The derivative of the BCE loss with respect to the Sigmoid output $y_{pred}$ is:\n    $ \\frac{\\partial L}{\\partial y_{pred}} = -\\left(\\frac{y_{true}}{y_{pred}} - \\frac{1 - y_{true}}{1 - y_{pred}}\\right) $\n\n2.  **Gradient of Output ($y_{pred}$) with respect to Output Weighted Sum ($Z_{output}$):**\n    The derivative of the Sigmoid function is $ \\sigma'(Z) = \\sigma(Z)(1 - \\sigma(Z)) $. So,\n    $ \\frac{\\partial y_{pred}}{\\partial Z_{output}} = y_{pred} (1 - y_{pred}) $\n\n3.  **Gradient of Loss with respect to Output Weighted Sum ($Z_{output}$):**\n    Using the chain rule:\n    $ \\frac{\\partial L}{\\partial Z_{output}} = \\frac{\\partial L}{\\partial y_{pred}} \\times \\frac{\\partial y_{pred}}{\\partial Z_{output}} = -\\left(\\frac{y_{true}}{y_{pred}} - \\frac{1 - y_{true}}{1 - y_{pred}}\\right) \\times y_{pred} (1 - y_{pred}) $\n    This simplifies nicely for BCE and Sigmoid:\n    $ \\frac{\\partial L}{\\partial Z_{output}} = y_{pred} - y_{true} $\n\n4.  **Gradient of Loss with respect to Output Layer Weights ($W_{output}$) and Bias ($B_{output}$):**\n    Now we can calculate the gradients for the output layer parameters using the chain rule:\n    $ \\frac{\\partial L}{\\partial W_{output}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial W_{output}} $\n    Since $Z_{output} = A_{hidden} \\cdot W_{output} + B_{output}$, the derivative of $Z_{output}$ with respect to $W_{output}$ is $A_{hidden}^T$.\n    $ \\frac{\\partial L}{\\partial W_{output}} = A_{hidden}^T \\cdot (y_{pred} - y_{true}) $ (Note: This is a matrix multiplication)\n\n    $ \\frac{\\partial L}{\\partial B_{output}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial B_{output}} $\n    The derivative of $Z_{output}$ with respect to $B_{output}$ is 1.\n    $ \\frac{\\partial L}{\\partial B_{output}} = y_{pred} - y_{true} $ (Summed over samples for the bias gradient)\n\n5.  **Gradient of Loss with respect to Hidden Layer Output ($A_{hidden}$):**\n    To propagate the gradient backward to the hidden layer, we need the gradient of the loss with respect to the hidden layer's output:\n    $ \\frac{\\partial L}{\\partial A_{hidden}} = \\frac{\\partial L}{\\partial Z_{output}} \\times \\frac{\\partial Z_{output}}{\\partial A_{hidden}} $\n    Since $Z_{output} = A_{hidden} \\cdot W_{output} + B_{output}$, the derivative of $Z_{output}$ with respect to $A_{hidden}$ is $W_{output}^T$.\n    $ \\frac{\\partial L}{\\partial A_{hidden}} = (y_{pred} - y_{true}) \\cdot W_{output}^T $ (Note: This is a matrix multiplication)\n\n6.  **Gradient of Hidden Layer Output ($A_{hidden}$) with respect to Hidden Layer Weighted Sum ($Z_{hidden}$):**\n    We used the ReLU activation function for the hidden layer. The derivative of the ReLU function is:\n    $ ReLU'(Z) = 1 $ if $ Z > 0 $\n    $ ReLU'(Z) = 0 $ if $ Z \\le 0 $\n    So, $ \\frac{\\partial A_{hidden}}{\\partial Z_{hidden}} = ReLU'(Z_{hidden}) $ (Element-wise multiplication)\n\n7.  **Gradient of Loss with respect to Hidden Layer Weighted Sum ($Z_{hidden}$):**\n    Using the chain rule:\n    $ \\frac{\\partial L}{\\partial Z_{hidden}} = \\frac{\\partial L}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial Z_{hidden}} = \\left((y_{pred} - y_{true}) \\cdot W_{output}^T\\right) \\times ReLU'(Z_{hidden}) $ (Element-wise multiplication with the derivative of ReLU)\n\n8.  **Gradient of Loss with respect to Hidden Layer Weights ($W_{hidden}$) and Bias ($B_{hidden}$):**\n    Finally, we can calculate the gradients for the hidden layer parameters:\n    $ \\frac{\\partial L}{\\partial W_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} \\times \\frac{\\partial Z_{hidden}}{\\partial W_{hidden}} $\n    Since $Z_{hidden} = X \\cdot W_{hidden} + B_{hidden}$, the derivative of $Z_{hidden}$ with respect to $W_{hidden}$ is $X^T$.\n    $ \\frac{\\partial L}{\\partial W_{hidden}} = X^T \\cdot \\left(\\frac{\\partial L}{\\partial Z_{hidden}}\\right) $ (Note: This is a matrix multiplication)\n\n    $ \\frac{\\partial L}{\\partial B_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} \\times \\frac{\\partial Z_{hidden}}{\\partial B_{hidden}} $\n    The derivative of $Z_{hidden}$ with respect to $B_{hidden}$ is 1.\n    $ \\frac{\\partial L}{\\partial B_{hidden}} = \\frac{\\partial L}{\\partial Z_{hidden}} $ (Summed over samples for the bias gradient)\n\n#### Gradient Descent Update Rule\n\nOnce we have the gradients for all weights and biases, we update them using the gradient descent rule:\n\n$ Parameter = Parameter - LearningRate \\times Gradient $\n\nWhere `Parameter` is a weight or bias, `LearningRate` is a small positive value that controls the step size, and `Gradient` is the calculated gradient of the loss with respect to that parameter.\n\nLet's implement these gradient calculations and the update rule. We'll need to modify our forward pass function slightly to store the intermediate values ($Z_{hidden}, A_{hidden}, Z_{output}$) which are needed for the backward pass.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e092118f"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the modified forward pass to store intermediate values and the backward pass to calculate gradients for all weights and biases, and finally implement the gradient descent update rule, covering instructions 3 and 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9e84b26",
        "outputId": "5fcc9511-bd89-4371-f2c9-8210cfb7c6ad"
      },
      "source": [
        "# Modify the forward pass to store intermediate values needed for backpropagation\n",
        "def forward_pass_with_intermediates(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
        "  \"\"\"\n",
        "  Represents the forward pass through a simple neural network,\n",
        "  returning intermediate values needed for backpropagation.\n",
        "\n",
        "  Args:\n",
        "    X: Input data (numpy array).\n",
        "    weights_hidden: Weights for the hidden layer.\n",
        "    bias_hidden: Bias for the hidden layer.\n",
        "    weights_output: Weights for the output layer.\n",
        "    bias_output: Bias for the output layer.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing:\n",
        "      - output_layer_output: The final output probability.\n",
        "      - hidden_layer_output: The output of the hidden layer (after activation).\n",
        "      - output_layer_input: The weighted sum for the output layer (before activation).\n",
        "      - hidden_layer_input: The weighted sum for the hidden layer (before activation).\n",
        "  \"\"\"\n",
        "  # Ensure X is a numpy array for matrix multiplication\n",
        "  X = np.array(X)\n",
        "\n",
        "  # Calculate the weighted sum for the hidden layer\n",
        "  hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden\n",
        "\n",
        "  # Apply the activation function for the hidden layer (using ReLU)\n",
        "  hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "  # Calculate the weighted sum for the output layer\n",
        "  output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
        "\n",
        "  # Apply the activation function for the output layer (using Sigmoid)\n",
        "  output_layer_output = sigmoid(output_layer_input)\n",
        "\n",
        "  return output_layer_output, hidden_layer_output, output_layer_input, hidden_layer_input\n",
        "\n",
        "# Implement the backward pass to calculate gradients\n",
        "def backward_pass(X, y_true, y_pred, hidden_layer_output, output_layer_input, weights_output, hidden_layer_input):\n",
        "  \"\"\"\n",
        "  Implements the backward pass to calculate gradients for weights and biases.\n",
        "\n",
        "  Args:\n",
        "    X: Input data (numpy array).\n",
        "    y_true: True binary labels (numpy array).\n",
        "    y_pred: Predicted probabilities from the forward pass (numpy array).\n",
        "    hidden_layer_output: Output of the hidden layer (after activation) from forward pass.\n",
        "    output_layer_input: Weighted sum for the output layer (before activation) from forward pass.\n",
        "    weights_output: Weights for the output layer.\n",
        "    hidden_layer_input: Weighted sum for the hidden layer (before activation) from forward pass.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing:\n",
        "      - grads_weights_hidden: Gradients for hidden layer weights.\n",
        "      - grads_bias_hidden: Gradients for hidden layer bias.\n",
        "      - grads_weights_output: Gradients for output layer weights.\n",
        "      - grads_bias_output: Gradients for output layer bias.\n",
        "  \"\"\"\n",
        "  m = X.shape[0] # Number of samples\n",
        "\n",
        "  # Calculate the gradient of the loss with respect to the output weighted sum (dZ_output)\n",
        "  # This is the simplified gradient for BCE with Sigmoid\n",
        "  dZ_output = y_pred - y_true.reshape(-1, 1) # Ensure y_true has compatible shape\n",
        "\n",
        "  # Calculate gradients for output layer weights and bias\n",
        "  grads_weights_output = np.dot(hidden_layer_output.T, dZ_output) / m\n",
        "  grads_bias_output = np.sum(dZ_output, axis=0) / m\n",
        "\n",
        "  # Calculate the gradient of the loss with respect to the hidden layer output (dA_hidden)\n",
        "  dA_hidden = np.dot(dZ_output, weights_output.T)\n",
        "\n",
        "  # Calculate the gradient of the loss with respect to the hidden layer weighted sum (dZ_hidden)\n",
        "  # Apply the derivative of the ReLU activation function\n",
        "  dZ_hidden = dA_hidden * (hidden_layer_input > 0) # Derivative of ReLU is 1 for Z > 0, 0 otherwise\n",
        "\n",
        "  # Calculate gradients for hidden layer weights and bias\n",
        "  grads_weights_hidden = np.dot(X.T, dZ_hidden) / m\n",
        "  grads_bias_hidden = np.sum(dZ_hidden, axis=0) / m\n",
        "\n",
        "  return grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output\n",
        "\n",
        "# Implement the gradient descent update rule\n",
        "def update_parameters(weights_hidden, bias_hidden, weights_output, bias_output,\n",
        "                      grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output,\n",
        "                      learning_rate):\n",
        "  \"\"\"\n",
        "  Updates the weights and biases using gradient descent.\n",
        "\n",
        "  Args:\n",
        "    weights_hidden: Current hidden layer weights.\n",
        "    bias_hidden: Current hidden layer bias.\n",
        "    weights_output: Current output layer weights.\n",
        "    bias_output: Current output layer bias.\n",
        "    grads_weights_hidden: Gradients for hidden layer weights.\n",
        "    grads_bias_hidden: Gradients for hidden layer bias.\n",
        "    grads_weights_output: Gradients for output layer weights.\n",
        "    grads_bias_output: Gradients for output layer bias.\n",
        "    learning_rate: The learning rate for gradient descent.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the updated weights and biases:\n",
        "      - weights_hidden_updated\n",
        "      - bias_hidden_updated\n",
        "      - weights_output_updated\n",
        "      - bias_output_updated\n",
        "  \"\"\"\n",
        "  weights_hidden_updated = weights_hidden - learning_rate * grads_weights_hidden\n",
        "  bias_hidden_updated = bias_hidden - learning_rate * grads_bias_hidden\n",
        "  weights_output_updated = weights_output - learning_rate * grads_weights_output\n",
        "  bias_output_updated = bias_output - learning_rate * grads_bias_output\n",
        "\n",
        "  return weights_hidden_updated, bias_hidden_updated, weights_output_updated, bias_output_updated\n",
        "\n",
        "# Demonstrate the backward pass and parameter update with a single forward pass example\n",
        "# Use the initialized weights and biases from the previous step\n",
        "# We need the intermediate outputs from the forward pass\n",
        "\n",
        "y_true_array = y.values # Convert pandas Series to numpy array for calculations\n",
        "\n",
        "# Perform a forward pass to get intermediates\n",
        "y_pred, hidden_layer_output, output_layer_input, hidden_layer_input = forward_pass_with_intermediates(\n",
        "    X, weights_hidden, bias_hidden, weights_output, bias_output\n",
        ")\n",
        "\n",
        "# Calculate gradients\n",
        "grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output = backward_pass(\n",
        "    X, y_true_array, y_pred, hidden_layer_output, output_layer_input, weights_output, hidden_layer_input\n",
        ")\n",
        "\n",
        "print(\"Gradients for Output Layer Weights:\\n\", grads_weights_output[:5])\n",
        "print(\"\\nGradients for Output Layer Bias:\\n\", grads_bias_output)\n",
        "print(\"\\nGradients for Hidden Layer Weights:\\n\", grads_weights_hidden[:5])\n",
        "print(\"\\nGradients for Hidden Layer Bias:\\n\", grads_bias_hidden)\n",
        "\n",
        "\n",
        "# Demonstrate parameter update\n",
        "learning_rate = 0.01\n",
        "weights_hidden_updated, bias_hidden_updated, weights_output_updated, bias_output_updated = update_parameters(\n",
        "    weights_hidden, bias_hidden, weights_output, bias_output,\n",
        "    grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output,\n",
        "    learning_rate\n",
        ")\n",
        "\n",
        "print(\"\\nOriginal Hidden Layer Weights:\\n\", weights_hidden[:5])\n",
        "print(\"\\nUpdated Hidden Layer Weights (first 5 rows):\\n\", weights_hidden_updated[:5])\n",
        "print(\"\\nOriginal Output Layer Weights:\\n\", weights_output[:5])\n",
        "print(\"\\nUpdated Output Layer Weights (first 5 rows):\\n\", weights_output_updated[:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for Output Layer Weights:\n",
            " [[0.02277063]\n",
            " [0.02336304]]\n",
            "\n",
            "Gradients for Output Layer Bias:\n",
            " [0.19156682]\n",
            "\n",
            "Gradients for Hidden Layer Weights:\n",
            " [[ 0.01178141  0.00692273]\n",
            " [-0.00226344 -0.00133   ]]\n",
            "\n",
            "Gradients for Hidden Layer Bias:\n",
            " [0.01333256 0.00783418]\n",
            "\n",
            "Original Hidden Layer Weights:\n",
            " [[0.02708323 0.04389714]\n",
            " [0.00784564 0.00253507]]\n",
            "\n",
            "Updated Hidden Layer Weights (first 5 rows):\n",
            " [[0.02696541 0.04382791]\n",
            " [0.00786827 0.00254837]]\n",
            "\n",
            "Original Output Layer Weights:\n",
            " [[0.06959742]\n",
            " [0.04089529]]\n",
            "\n",
            "Updated Output Layer Weights (first 5 rows):\n",
            " [[0.06936971]\n",
            " [0.04066166]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5954ba5a"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the forward pass, loss calculation, backward pass, and parameter updates into a training loop, train the network for a specified number of epochs, and print the loss periodically, covering instructions 5, 6, and 7.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28af5789",
        "outputId": "0d7cb4f8-6cb6-492d-fa1d-db668673ed70"
      },
      "source": [
        "# 5. Combine the steps into a training loop\n",
        "# 6. Train the neural network on the dataset X and y for a specified number of epochs\n",
        "# 7. Print the loss periodically during training to show the learning progress\n",
        "\n",
        "# Initialize weights and biases again (or use the updated ones from the previous step)\n",
        "# Let's re-initialize for a fresh training run demonstration\n",
        "weights_hidden = np.random.rand(n_input_features, n_hidden_neurons) * 0.1\n",
        "bias_hidden = np.random.rand(n_hidden_neurons) * 0.1\n",
        "weights_output = np.random.rand(n_hidden_neurons, n_output_neurons) * 0.1\n",
        "bias_output = np.random.rand(n_output_neurons) * 0.1\n",
        "\n",
        "learning_rate = 0.1 # Set a learning rate\n",
        "epochs = 1000 # Define the number of training epochs\n",
        "\n",
        "# Convert y to numpy array for consistent calculations\n",
        "y_true_array = y.values.reshape(-1, 1)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "  # Forward pass to get predictions and intermediate values\n",
        "  y_pred, hidden_layer_output, output_layer_input, hidden_layer_input = forward_pass_with_intermediates(\n",
        "      X, weights_hidden, bias_hidden, weights_output, bias_output\n",
        "  )\n",
        "\n",
        "  # Calculate the loss\n",
        "  loss = binary_cross_entropy_loss(y_true_array, y_pred)\n",
        "\n",
        "  # Backward pass to calculate gradients\n",
        "  grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output = backward_pass(\n",
        "      X, y_true_array, y_pred, hidden_layer_output, output_layer_input, weights_output, hidden_layer_input\n",
        "  )\n",
        "\n",
        "  # Update parameters using gradient descent\n",
        "  weights_hidden, bias_hidden, weights_output, bias_output = update_parameters(\n",
        "      weights_hidden, bias_hidden, weights_output, bias_output,\n",
        "      grads_weights_hidden, grads_bias_hidden, grads_weights_output, grads_bias_output,\n",
        "      learning_rate\n",
        "  )\n",
        "\n",
        "  # Print the loss periodically\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Optionally, print the final loss\n",
        "print(f\"Final Loss after {epochs} epochs: {loss:.4f}\")\n",
        "\n",
        "# Optionally, make predictions after training\n",
        "final_predictions, _, _, _ = forward_pass_with_intermediates(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
        "binary_final_predictions = (final_predictions >= 0.5).astype(int)\n",
        "\n",
        "# You can compare binary_final_predictions with y_true_array to see accuracy\n",
        "# For demonstration, let's just print the first few predictions\n",
        "print(\"\\nFirst 10 binary predictions after training:\")\n",
        "print(binary_final_predictions[:10].flatten())\n",
        "print(\"First 10 true labels:\")\n",
        "print(y_true_array[:10].flatten())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 0, Loss: 0.7059\n",
            "Epoch 100, Loss: 0.5369\n",
            "Epoch 200, Loss: 0.4649\n",
            "Epoch 300, Loss: 0.4243\n",
            "Epoch 400, Loss: 0.3987\n",
            "Epoch 500, Loss: 0.3864\n",
            "Epoch 600, Loss: 0.3799\n",
            "Epoch 700, Loss: 0.3773\n",
            "Epoch 800, Loss: 0.3768\n",
            "Epoch 900, Loss: 0.4023\n",
            "Training finished.\n",
            "Final Loss after 1000 epochs: 0.3737\n",
            "\n",
            "First 10 binary predictions after training:\n",
            "[0 1 1 0 0 0 0 1 0 0]\n",
            "First 10 true labels:\n",
            "[0 1 0 0 0 0 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab96495f"
      },
      "source": [
        "## Neural Network Evaluation\n",
        "\n",
        "### Evaluate the performance of the trained neural network by using the trained weights and biases to make predictions, convert them to binary, and calculate the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b156e2",
        "outputId": "20295d95-85ca-454f-8e5a-cac83afe8c61"
      },
      "source": [
        "# 1. Use the trained weights and biases to make predictions on the input data X\n",
        "# We use the final weights and biases obtained after training\n",
        "final_predictions, _, _, _ = forward_pass_with_intermediates(\n",
        "    X, weights_hidden, bias_hidden, weights_output, bias_output\n",
        ")\n",
        "\n",
        "# 2. Convert the predicted probabilities into binary predictions (0 or 1) using a threshold of 0.5\n",
        "threshold = 0.5\n",
        "binary_predictions = (final_predictions >= threshold).astype(int)\n",
        "\n",
        "# 3. Calculate the accuracy of the model\n",
        "# Ensure y is a numpy array for comparison\n",
        "y_true_array = y.values.reshape(-1, 1)\n",
        "\n",
        "# Compare binary predictions to true labels\n",
        "correct_predictions = (binary_predictions == y_true_array).sum()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / y_true_array.shape[0]\n",
        "\n",
        "# 4. Print the calculated accuracy\n",
        "print(f\"Accuracy of the trained neural network: {accuracy:.4f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the trained neural network: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "417221a1"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2be6bf70",
        "outputId": "4a98c60f-c85b-4113-9d6a-5cd5ff40c415"
      },
      "source": [
        "%%markdown\n",
        "## Summary and Next Steps\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "In this notebook, we built a simple neural network from scratch, covering the fundamental concepts:\n",
        "\n",
        "*   **The Neuron:** The basic computational unit, performing a weighted sum of inputs and applying an activation function.\n",
        "*   **Weighted Sum and Bias:** How inputs are combined with weights and a bias term ($Z = \\sum (input_i \\times weight_i) + bias$).\n",
        "*   **Activation Functions:** The role of non-linear functions (like Sigmoid, ReLU, Tanh) in enabling the network to learn complex patterns.\n",
        "*   **Layers:** The organization of neurons into Input, Hidden, and Output layers.\n",
        "*   **Input Layer:** Receiving and structuring the initial data.\n",
        "*   **Output Layer:** Producing the final prediction, with the activation function chosen based on the task (Sigmoid for binary classification).\n",
        "*   **Hidden Layers:** Learning abstract representations of the data through non-linear transformations.\n",
        "*   **Forward Pass:** The process of feeding data through the network to get a prediction.\n",
        "*   **Loss Function:** Measuring the error between predictions and true labels (Binary Cross-Entropy for our task).\n",
        "*   **Backward Pass (Backpropagation):** Calculating the gradient of the loss with respect to each weight and bias using the chain rule.\n",
        "*   **Gradient Descent:** An optimization algorithm for updating weights and biases in the direction that minimizes the loss.\n",
        "*   **Training Loop:** The iterative process of forward pass, loss calculation, backward pass, and parameter updates over multiple epochs.\n",
        "*   **Evaluation:** Assessing the trained network's performance using metrics like accuracy.\n",
        "\n",
        "By coding these components ourselves, we gained a deeper understanding of how neural networks work internally.\n",
        "\n",
        "### Where to Go From Here\n",
        "\n",
        "This notebook provided a foundation. Neural networks are a vast field with many avenues for further exploration. Here are some suggested next steps:\n",
        "\n",
        "*   **Experiment with Network Architecture:**\n",
        "    *   Add more hidden layers to create a deeper network.\n",
        "    *   Vary the number of neurons in the hidden layer(s).\n",
        "*   **Try Different Activation Functions:**\n",
        "    *   Replace ReLU with Leaky ReLU, ELU, or Swish in the hidden layers.\n",
        "    *   Explore Softmax for multi-class classification problems.\n",
        "*   **Implement Other Optimization Algorithms:**\n",
        "    *   Research and implement more advanced optimizers like Adam, RMSprop, or Adagrad, which often converge faster and more effectively than basic Gradient Descent.\n",
        "*   **Explore Different Training Strategies:**\n",
        "    *   Implement **Batch Gradient Descent** (using mini-batches of data for updates).\n",
        "    *   Implement **Stochastic Gradient Descent (SGD)** (updating parameters after each single data point).\n",
        "*   **Regularization Techniques:**\n",
        "    *   Learn about techniques like L1/L2 regularization or Dropout to prevent overfitting.\n",
        "*   **Work with More Complex Datasets:**\n",
        "    *   Apply your knowledge to larger and more complex datasets (e.g., image datasets like MNIST or CIFAR-10, text datasets). This will likely require using more sophisticated libraries or frameworks, but the core principles you've learned here will still apply.\n",
        "*   **Implement Other Network Types:**\n",
        "    *   Explore Convolutional Neural Networks (CNNs) for image data.\n",
        "    *   Explore Recurrent Neural Networks (RNNs) for sequential data.\n",
        "*   **Use Deep Learning Frameworks:**\n",
        "    *   Transition to using popular deep learning libraries like TensorFlow or PyTorch. While coding from scratch is invaluable for understanding, these frameworks significantly simplify building, training, and deploying complex neural networks. You'll find that the concepts (layers, activations, loss functions, optimizers) are the same, but the implementation is much more streamlined.\n",
        "\n",
        "Continuing to experiment and build will solidify your understanding and open up the possibilities of what you can achieve with neural networks!"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Summary and Next Steps\n\n### What We Learned\n\nIn this notebook, we built a simple neural network from scratch, covering the fundamental concepts:\n\n*   **The Neuron:** The basic computational unit, performing a weighted sum of inputs and applying an activation function.\n*   **Weighted Sum and Bias:** How inputs are combined with weights and a bias term ($Z = \\sum (input_i \\times weight_i) + bias$).\n*   **Activation Functions:** The role of non-linear functions (like Sigmoid, ReLU, Tanh) in enabling the network to learn complex patterns.\n*   **Layers:** The organization of neurons into Input, Hidden, and Output layers.\n*   **Input Layer:** Receiving and structuring the initial data.\n*   **Output Layer:** Producing the final prediction, with the activation function chosen based on the task (Sigmoid for binary classification).\n*   **Hidden Layers:** Learning abstract representations of the data through non-linear transformations.\n*   **Forward Pass:** The process of feeding data through the network to get a prediction.\n*   **Loss Function:** Measuring the error between predictions and true labels (Binary Cross-Entropy for our task).\n*   **Backward Pass (Backpropagation):** Calculating the gradient of the loss with respect to each weight and bias using the chain rule.\n*   **Gradient Descent:** An optimization algorithm for updating weights and biases in the direction that minimizes the loss.\n*   **Training Loop:** The iterative process of forward pass, loss calculation, backward pass, and parameter updates over multiple epochs.\n*   **Evaluation:** Assessing the trained network's performance using metrics like accuracy.\n\nBy coding these components ourselves, we gained a deeper understanding of how neural networks work internally.\n\n### Where to Go From Here\n\nThis notebook provided a foundation. Neural networks are a vast field with many avenues for further exploration. Here are some suggested next steps:\n\n*   **Experiment with Network Architecture:**\n    *   Add more hidden layers to create a deeper network.\n    *   Vary the number of neurons in the hidden layer(s).\n*   **Try Different Activation Functions:**\n    *   Replace ReLU with Leaky ReLU, ELU, or Swish in the hidden layers.\n    *   Explore Softmax for multi-class classification problems.\n*   **Implement Other Optimization Algorithms:**\n    *   Research and implement more advanced optimizers like Adam, RMSprop, or Adagrad, which often converge faster and more effectively than basic Gradient Descent.\n*   **Explore Different Training Strategies:**\n    *   Implement **Batch Gradient Descent** (using mini-batches of data for updates).\n    *   Implement **Stochastic Gradient Descent (SGD)** (updating parameters after each single data point).\n*   **Regularization Techniques:**\n    *   Learn about techniques like L1/L2 regularization or Dropout to prevent overfitting.\n*   **Work with More Complex Datasets:**\n    *   Apply your knowledge to larger and more complex datasets (e.g., image datasets like MNIST or CIFAR-10, text datasets). This will likely require using more sophisticated libraries or frameworks, but the core principles you've learned here will still apply.\n*   **Implement Other Network Types:**\n    *   Explore Convolutional Neural Networks (CNNs) for image data.\n    *   Explore Recurrent Neural Networks (RNNs) for sequential data.\n*   **Use Deep Learning Frameworks:**\n    *   Transition to using popular deep learning libraries like TensorFlow or PyTorch. While coding from scratch is invaluable for understanding, these frameworks significantly simplify building, training, and deploying complex neural networks. You'll find that the concepts (layers, activations, loss functions, optimizers) are the same, but the implementation is much more streamlined.\n\nContinuing to experiment and build will solidify your understanding and open up the possibilities of what you can achieve with neural networks!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd7833fc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A synthetic dataset with 100 samples and 2 features (`feature1`, `feature2`) was generated for binary classification, with a target variable (`target`) determined by a simple rule with added noise.\n",
        "*   A single neuron's core components (inputs, weights, bias, weighted sum, activation function) were explained and implemented, demonstrating the calculation of the weighted sum and applying a step activation function.\n",
        "*   The role of the input layer was explained, emphasizing the importance of matching the number of input neurons to the number of features in the data (2 in this case). The shape of the input data `X` was confirmed to be (100, 2).\n",
        "*   Several common activation functions (Sigmoid, ReLU, Tanh) were explained mathematically and implemented in Python, demonstrating their different output behaviors for various input values.\n",
        "*   The output layer's function was explained, highlighting the selection of the appropriate activation function based on the task (Sigmoid for binary classification). The concept of applying a threshold (e.g., 0.5) to the Sigmoid output for binary prediction was demonstrated, showing how changing the threshold can alter the prediction.\n",
        "*   The purpose of hidden layers in learning complex patterns was explained, stressing the necessity of non-linear activation functions. The calculation within a hidden neuron, including the weighted sum and applying ReLU and Tanh activations, was demonstrated.\n",
        "*   A simple neural network architecture with one hidden layer (2 neurons) and an output layer (1 neuron) was defined. The forward pass was implemented, showing how data flows through the layers, applying weighted sums and activation functions. The dimensions of initialized weights and biases were confirmed to match the network structure.\n",
        "*   The training process was explained, introducing the Binary Cross-Entropy loss function for binary classification and the Gradient Descent optimization algorithm. The backward pass (backpropagation) was explained as the method for calculating gradients using the chain rule.\n",
        "*   The Binary Cross-Entropy loss function, a modified forward pass to store intermediate values, the backward pass for gradient calculation, and the parameter update rule (Gradient Descent) were implemented.\n",
        "*   A training loop combining the forward pass, loss calculation, backward pass, and parameter updates was executed for 1000 epochs. The periodic printing of the loss showed a decrease over epochs, indicating that the network was learning.\n",
        "*   The trained network's performance was evaluated by calculating its accuracy on the training data, achieving an accuracy of 0.9000.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The notebook successfully guided the user through building and training a simple neural network from scratch, illustrating the fundamental concepts and their implementation.\n",
        "*   Next steps could involve exploring more advanced concepts like different optimizers (Adam, RMSprop), regularization techniques (Dropout, L1/L2), different network architectures (more layers, varying neuron counts), and applying the learned principles to more complex datasets.\n"
      ]
    }
  ]
}