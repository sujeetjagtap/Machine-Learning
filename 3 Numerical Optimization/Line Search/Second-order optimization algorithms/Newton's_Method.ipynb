{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo4iIFgaaLAT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "021495e6"
      },
      "source": [
        "This cell imports the necessary libraries for numerical computations. `numpy` is imported as `np`, which is a standard convention. The `inv` function from `numpy.linalg` is specifically imported to calculate the inverse of a matrix, a crucial step in Newton's method. These imports provide the mathematical tools needed for implementing the optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIgHxNpsaLAW"
      },
      "outputs": [],
      "source": [
        "#The objective function\n",
        "def func(x):\n",
        "    return 100*np.square(np.square(x[0])-x[1])+np.square(x[0]-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07a81ec9"
      },
      "source": [
        "This cell defines the Rosenbrock function, a non-convex function used as a performance test problem for optimization algorithms. The function is given by `f(x, y) = 100 * (y - x^2)^2 + (x - 1)^2`. The goal of the optimization is to find the values of `x` and `y` that minimize this function. The global minimum of the Rosenbrock function is at `(1, 1)`, where the function value is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qI89Zf-aLAX"
      },
      "outputs": [],
      "source": [
        "# first order derivatives of the function\n",
        "def dfunc(x):\n",
        "    df1 = 400*x[0]*(np.square(x[0])-x[1])+2*(x[0]-1)\n",
        "    df2 = -200*(np.square(x[0])-x[1])\n",
        "    return np.array([df1, df2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24c38da5"
      },
      "source": [
        "This cell defines the gradient of the Rosenbrock function. The gradient is a vector containing the partial derivatives of the function with respect to each variable (`x[0]` and `x[1]`). These derivatives are calculated analytically:\n",
        "- The partial derivative with respect to `x[0]` is `400*x[0]*(x[0]^2 - x[1]) + 2*(x[0] - 1)`.\n",
        "- The partial derivative with respect to `x[1]` is `-200*(x[0]^2 - x[1])`.\n",
        "The gradient points in the direction of the steepest increase of the function. In optimization, we move in the opposite direction of the gradient to find the minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "019HxOQJaLAY"
      },
      "outputs": [],
      "source": [
        "# second order derivative: the hessian\n",
        "def invhess(x):\n",
        "    df11 = 1200*np.square(x[0])-400*x[1]+2\n",
        "    df12 = -400*x[0]\n",
        "    df21 = -400*x[0]\n",
        "    df22 = 200\n",
        "    hess = np.array([[df11, df12], [df21, df22]])\n",
        "    return inv(hess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1c7aa1"
      },
      "source": [
        "This cell calculates the inverse of the Hessian matrix of the Rosenbrock function. The Hessian matrix is a square matrix of second-order partial derivatives. For a function of two variables like the Rosenbrock function, the Hessian is a 2x2 matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGRipBg5aLAZ"
      },
      "outputs": [],
      "source": [
        "#The method\n",
        "def newton(x, max_int):\n",
        "    miter = 1\n",
        "    step = .5\n",
        "    vals = []\n",
        "    objectfs = []\n",
        "    # you can customize your own condition of convergence, here we limit the number of iterations\n",
        "    while miter <= max_int:\n",
        "        vals.append(x)\n",
        "        objectfs.append(func(x))\n",
        "        temp = x-step*(invhess(x).dot(dfunc(x)))\n",
        "        if np.abs(func(temp)-func(x))>0.01:\n",
        "            x = temp\n",
        "        else:\n",
        "            break\n",
        "        print(x, func(x), miter)\n",
        "        miter += 1\n",
        "    return vals, objectfs, miter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90614e13"
      },
      "source": [
        "This cell defines the `newton` function, which implements Newton's method for optimization.\n",
        "- `x`: The initial guess for the minimum.\n",
        "- `max_int`: The maximum number of iterations allowed.\n",
        "The function iteratively updates the current estimate of the minimum using the formula:\n",
        "`x_new = x_old - step * (H_inv * gradient)`\n",
        "where `H_inv` is the inverse Hessian and `gradient` is the gradient of the function at the current point.\n",
        "- `miter`: Counter for the current iteration.\n",
        "- `step`: A step size parameter (set to 0.5). In a pure Newton's method, the step size is typically 1, but a smaller step size can help with convergence in some cases.\n",
        "- `vals`: A list to store the values of `x` at each iteration.\n",
        "- `objectfs`: A list to store the objective function values at each iteration.\n",
        "The `while` loop continues until the maximum number of iterations is reached or the absolute difference between the objective function value at the current and next step is less than 0.01, which serves as a basic convergence criterion. The current value of `x`, the objective function value, and the iteration number are printed in each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjNDIB5saLAa",
        "outputId": "230296d0-e21c-493e-a1da-8ae711268960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 4.99950012 14.99500125] 10015.996500999738 1\n",
            "[ 4.99850075 19.98500862] 2515.9891319336307 2\n",
            "[ 4.9965035  22.46504264] 640.9743156346814 3\n",
            "[ 4.99251498 23.67518762] 172.19472181001353 4\n",
            "[ 4.98456188 24.22078475] 54.94827682735826 5\n",
            "[ 4.96875194 24.37570969] 25.534508005533738 6\n",
            "[ 4.93753006 24.22183522] 17.980607577859715 7\n",
            "[ 4.87690338 23.70182702] 15.708688997277847 8\n",
            "[ 4.7659566  22.66085333] 14.468535895722859 9\n",
            "[ 4.60498788 21.153258  ] 13.273196783901964 10\n",
            "[ 4.44867175 19.7399179 ] 12.151019317491137 11\n",
            "[ 4.29405728 18.38964106] 11.093732736283506 12\n",
            "[ 4.14236045 17.11149476] 10.101532481072594 13\n",
            "[ 3.99316572 15.89928575] 9.171439844130546 14\n",
            "[ 3.84669105 14.7525338 ] 8.301658919062707 15\n",
            "[ 3.70291355 13.6686477 ] 7.489963487430327 16\n",
            "[ 3.56190493 12.64582277] 6.734289227351629 17\n",
            "[ 3.42370436 11.6819802 ] 6.032519109543599 18\n",
            "[ 3.28836654 10.77515251] 5.382560838831825 19\n",
            "[3.1559441  9.92334647] 4.782319830836418 20\n",
            "[3.02649439 9.12459271] 4.229709167267734 21\n",
            "[2.90007737 8.37692968] 3.7226467068415037 22\n",
            "[2.77675676 7.67841059] 3.259056681523529 23\n",
            "[2.65660008 7.02710263] 2.836869850303512 24\n",
            "[2.53967916 6.42108902] 2.4540242239312926 25\n",
            "[2.42607046 5.85847036] 2.1084657085379117 26\n",
            "[2.31585568 5.33736646] 1.7981489002130808 27\n",
            "[2.20912221 4.85591835] 1.5210379805225525 28\n",
            "[2.10596382 4.41229066] 1.2751077585294368 29\n",
            "[2.00648136 4.00467423] 1.058344882856319 30\n",
            "[1.91078358 3.63128922] 0.8687492640582765 31\n",
            "[1.81898804 3.29038872] 0.7043357559087118 32\n",
            "[1.73122212 2.9802628 ] 0.5631361595274927 33\n",
            "[1.64762421 2.69924331] 0.44320163392601775 34\n",
            "[1.56834489 2.44570936] 0.34260562386752896 35\n",
            "[1.4935482  2.21809352] 0.2594474537928738 36\n",
            "[1.42341286 2.01488885] 0.19185678962341826 37\n",
            "[1.3581331  1.83465641] 0.13799924506512215 38\n",
            "[1.29791881 1.67603292] 0.09608351458913175 39\n",
            "[1.24299405 1.53773732] 0.06437056260653057 40\n",
            "[1.19359251 1.41857412] 0.04118559716842384 41\n",
            "[1.14994716 1.31742909] 0.024933802008029717 42\n",
            "[1.1122697 1.2332496] 0.014121033914283581 43\n"
          ]
        }
      ],
      "source": [
        "#Initialization\n",
        "start = [5, 5]\n",
        "val, objectf, iters = newton(start, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "177552d8"
      },
      "source": [
        "This cell initializes the optimization process.\n",
        "- `start = [5, 5]`: Sets the initial guess for the minimum to `(5, 5)`. The performance of optimization algorithms can sometimes depend on the starting point.\n",
        "- `val, objectf, iters = newton(start, 50)`: Calls the `newton` function with the initial guess and a maximum of 50 iterations. The returned values are:\n",
        "    - `val`: A list of the `x` values at each iteration.\n",
        "    - `objectf`: A list of the objective function values at each iteration.\n",
        "    - `iters`: The total number of iterations performed until convergence or the maximum number of iterations was reached.\n",
        "This cell executes the Newton's method and stores the results for further analysis or visualization. The output shows the progress of the optimization at each step, including the current `x` values, the objective function value, and the iteration number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904b8d54"
      },
      "source": [
        "# Question\n",
        "How Newton's method can be applied to the IRIS dataset for classification by framing it as an optimization problem using Logistic Regression, and provide an assignment for the user to implement this along with an example Python code response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6663f1a"
      },
      "source": [
        "## The challenge\n",
        "\n",
        "Why directly applying Newton's method for optimization (finding minima of a function) to a classification problem like the IRIS dataset isn't straightforward. Also Newton's method is typically used for minimizing continuous, differentiable functions, while classification involves discrete categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fe7a47b"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain why Newton's method is not directly applicable to classification problems like the IRIS dataset by highlighting the difference between optimizing continuous functions and predicting discrete categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcfae097"
      },
      "source": [
        "# Explanation of why Newton's method is not directly applicable to classification\n",
        "\n",
        "# Newton's method is an iterative optimization algorithm designed to find the roots of a differentiable function or the minima/maxima of a twice-differentiable function.\n",
        "# It relies on calculating the gradient and Hessian (matrix of second derivatives) of the objective function.\n",
        "# Therefore, it is inherently suited for minimizing continuous and differentiable functions.\n",
        "\n",
        "# Classification problems, such as classifying the IRIS dataset, involve predicting discrete categories or labels (e.g., 'setosa', 'versicolor', 'virginica').\n",
        "# The direct performance metrics in classification, like accuracy or the number of misclassified points, are not continuous or differentiable functions of the model parameters.\n",
        "# Small changes in model parameters might not change the classification of any point until a certain threshold is crossed, leading to a step-like, non-differentiable objective function.\n",
        "\n",
        "# Hence, directly applying Newton's method to minimize a non-differentiable function that counts misclassifications is not feasible.\n",
        "\n",
        "# To apply Newton's method to a classification problem, the problem needs to be reframed as an optimization task with a continuous and differentiable objective function.\n",
        "# Logistic Regression is an example of such a reformulation, where the classification problem is solved by minimizing a cost function (like the negative log-likelihood) which is continuous and differentiable, making it suitable for optimization methods like Newton's method."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59fbeb9b"
      },
      "source": [
        "## Introduce a suitable model\n",
        "\n",
        "To use an optimization method like Newton's, we need to frame the classification problem as an optimization problem by choosing a model with a differentiable loss function. Suggest using Logistic Regression as a suitable model for binary classification, as its loss function (cross-entropy) is differentiable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ab3b7f4"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain how to frame classification as an optimization problem suitable for Newton's method using Logistic Regression and its differentiable cross-entropy loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5665ebdb"
      },
      "source": [
        "# To apply an optimization method like Newton's method to a classification problem,\n",
        "# we need to reformulate the problem as minimizing a continuous and differentiable\n",
        "# function of the model parameters. This function is typically called the \"loss function\"\n",
        "# or \"cost function\".\n",
        "\n",
        "# For binary classification problems, Logistic Regression is a suitable model choice.\n",
        "# Logistic Regression models the probability of a data point belonging to a particular class\n",
        "# using the sigmoid function, which is continuous and differentiable.\n",
        "\n",
        "# The standard loss function used in Logistic Regression is the cross-entropy loss (also known as log loss).\n",
        "# The cross-entropy loss measures the performance of a classification model whose output is a probability value\n",
        "# between 0 and 1. For binary classification, the cross-entropy loss is defined as:\n",
        "# L = -[y * log(p) + (1 - y) * log(1 - p)]\n",
        "# where y is the true label (0 or 1) and p is the predicted probability.\n",
        "\n",
        "# The key property of the cross-entropy loss function is that it is continuous and differentiable\n",
        "# with respect to the model parameters (the weights and bias). This differentiability is crucial\n",
        "# because Newton's method requires the calculation of the gradient (first derivative) and the\n",
        "# Hessian (second derivative) of the objective function.\n",
        "\n",
        "# By minimizing the cross-entropy loss function using optimization algorithms like Newton's method,\n",
        "# we can find the optimal model parameters for the Logistic Regression model that best fit the data\n",
        "# and can be used for classification."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b35107"
      },
      "source": [
        "## The Activity\n",
        "\n",
        "Loading and preparing the IRIS dataset (focusing on a binary classification problem, e.g., distinguishing two classes), implementing Logistic Regression from scratch, using Newton's method to find the optimal parameters for the Logistic Regression model by minimizing the cross-entropy loss function, and evaluating the performance of the implemented model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215fc9cb"
      },
      "source": [
        "# Assignment: Implement Logistic Regression with Newton's Method for Binary Classification on the IRIS Dataset\n",
        "\n",
        "# This assignment requires you to apply the concepts of framing classification as an optimization problem\n",
        "# and using Newton's method to find the optimal parameters for a Logistic Regression model.\n",
        "\n",
        "# Your task is to:\n",
        "\n",
        "# 1.  Load and Prepare the Data:\n",
        "#     - Load the IRIS dataset using libraries like scikit-learn or pandas.\n",
        "#     - Focus on a binary classification problem. For instance, select two classes from the dataset\n",
        "#       (e.g., 'setosa' and 'versicolor') and filter the data accordingly.\n",
        "#     - Extract the features (X) and the corresponding labels (y) for the selected classes.\n",
        "#     - Convert the labels to a binary format (e.g., 0 and 1).\n",
        "#     - Consider adding a bias term to your feature matrix (a column of ones).\n",
        "\n",
        "# 2.  Implement Logistic Regression from Scratch:\n",
        "#     - Implement the sigmoid function, which is the core of Logistic Regression for predicting probabilities.\n",
        "#     - Implement the prediction function that uses the sigmoid function and the model parameters (weights and bias)\n",
        "#       to predict the probability of a data point belonging to the positive class.\n",
        "\n",
        "# 3.  Implement the Cross-Entropy Loss Function:\n",
        "#     - Implement the binary cross-entropy loss function to quantify the error of your model's predictions.\n",
        "#     - This will be the function you aim to minimize.\n",
        "\n",
        "# 4.  Implement the Gradient and Hessian of the Loss Function:\n",
        "#     - Analytically derive and implement the gradient (vector of first derivatives) of the cross-entropy loss\n",
        "#       function with respect to the model parameters.\n",
        "#     - Analytically derive and implement the Hessian (matrix of second derivatives) of the cross-entropy loss\n",
        "#       function with respect to the model parameters. These are crucial for Newton's method.\n",
        "\n",
        "# 5.  Implement Newton's Method for Optimization:\n",
        "#     - Implement Newton's method to find the optimal parameters (weights and bias) that minimize the\n",
        "#       cross-entropy loss function.\n",
        "#     - Start with an initial guess for the parameters.\n",
        "#     - In each iteration, calculate the gradient and the inverse of the Hessian at the current parameter values.\n",
        "#     - Update the parameters using the Newton update rule: parameters = parameters - inverse(Hessian) * gradient.\n",
        "#     - Define a convergence criterion (e.g., based on the change in parameters, the change in loss, or a maximum number of iterations).\n",
        "\n",
        "# 6.  Evaluate the Model:\n",
        "#     - After finding the optimal parameters using Newton's method, evaluate the performance of your Logistic Regression model\n",
        "#       on the prepared binary IRIS dataset.\n",
        "#     - Calculate relevant classification metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "# By completing this assignment, you will gain practical experience in applying optimization techniques\n",
        "# to machine learning models and understand the connection between classification and optimization."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b332f9d0"
      },
      "source": [
        "## The Code\n",
        "\n",
        "This code demonstrates: loading and preparing the data, implementing Logistic Regression components (sigmoid, prediction, loss, gradient, Hessian), implementing Newton's method, training the model, and evaluating the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e4c7b0b",
        "outputId": "ed7f7131-f578-47ce-e480-abfe50031e72"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load the iris dataset and select two classes\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Select two classes (e.g., setosa (0) and versicolor (1))\n",
        "X_binary = X[y <= 1]\n",
        "y_binary = y[y <= 1]\n",
        "\n",
        "# 2. Prepare the data: convert labels to 0 and 1, and add a bias term\n",
        "y_binary = np.where(y_binary == 0, 0, 1) # Ensure labels are 0 and 1\n",
        "\n",
        "# Add a bias term (column of ones) to the feature matrix\n",
        "X_binary = np.hstack((np.ones((X_binary.shape[0], 1)), X_binary))\n",
        "\n",
        "# Split data into training and testing sets (optional but good practice)\n",
        "# For this example, we'll train on the full binary dataset as per the prompt\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use the full binary dataset for training and evaluation as per the prompt\n",
        "X_train = X_binary\n",
        "y_train = y_binary\n",
        "\n",
        "print(\"Data loaded and prepared:\")\n",
        "print(\"Features shape:\", X_train.shape)\n",
        "print(\"Labels shape:\", y_train.shape)\n",
        "print(\"First 5 rows of features (with bias):\")\n",
        "print(X_train[:5])\n",
        "print(\"First 5 labels:\")\n",
        "print(y_train[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and prepared:\n",
            "Features shape: (100, 5)\n",
            "Labels shape: (100,)\n",
            "First 5 rows of features (with bias):\n",
            "[[1.  5.1 3.5 1.4 0.2]\n",
            " [1.  4.9 3.  1.4 0.2]\n",
            " [1.  4.7 3.2 1.3 0.2]\n",
            " [1.  4.6 3.1 1.5 0.2]\n",
            " [1.  5.  3.6 1.4 0.2]]\n",
            "First 5 labels:\n",
            "[0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea91d9f1"
      },
      "source": [
        "Now that the data is loaded and prepared, implement the core components of the Logistic Regression model: the sigmoid function, the prediction function, the binary cross-entropy loss function, its gradient, and its Hessian.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9658e69b",
        "outputId": "ab5b0022-711d-47a4-98b4-732439ceaccb"
      },
      "source": [
        "# 3. Implement the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# 4. Implement the prediction function for logistic regression\n",
        "def predict(X, weights):\n",
        "    # X is the feature matrix (with bias)\n",
        "    # weights is the vector of model parameters (including bias)\n",
        "    z = X.dot(weights)\n",
        "    return sigmoid(z)\n",
        "\n",
        "# 5. Implement the binary cross-entropy loss function\n",
        "def cross_entropy_loss(X, y, weights):\n",
        "    m = X.shape[0] # Number of training examples\n",
        "    predictions = predict(X, weights)\n",
        "    # Avoid log(0) by clipping probabilities\n",
        "    predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
        "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
        "    return loss\n",
        "\n",
        "# 6. Implement the gradient of the cross-entropy loss function\n",
        "def gradient(X, y, weights):\n",
        "    m = X.shape[0]\n",
        "    predictions = predict(X, weights)\n",
        "    grad = (1/m) * X.T.dot(predictions - y)\n",
        "    return grad\n",
        "\n",
        "# 7. Implement the Hessian of the cross-entropy loss function\n",
        "def hessian(X, weights):\n",
        "    m = X.shape[0]\n",
        "    predictions = predict(X, weights)\n",
        "    # W is a diagonal matrix where W_ii = p_i * (1 - p_i)\n",
        "    W = np.diag(predictions * (1 - predictions))\n",
        "    hess = (1/m) * X.T.dot(W).dot(X)\n",
        "    return hess\n",
        "\n",
        "print(\"Logistic Regression components implemented.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression components implemented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7165d682"
      },
      "source": [
        "With the Logistic Regression components defined, now implement Newton's method for optimizing the model parameters, train the model using this method, and finally evaluate the trained model on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952d2f1f",
        "outputId": "b248ed92-4892-4971-ebcc-5baf4f297798"
      },
      "source": [
        "# 8. Implement Newton's method for optimizing the model parameters\n",
        "def newtons_method(X, y, initial_weights, max_iterations=100, tolerance=1e-6):\n",
        "    weights = initial_weights\n",
        "    loss_history = []\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad = gradient(X, y, weights)\n",
        "        hess = hessian(X, weights)\n",
        "\n",
        "        # Handle potential singularity or near-singularity of Hessian\n",
        "        try:\n",
        "            # Use pseudo-inverse for stability\n",
        "            hess_inv = np.linalg.pinv(hess)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(f\"Hessian inversion failed at iteration {i}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        # Newton's update rule\n",
        "        update = hess_inv.dot(grad)\n",
        "        weights -= update\n",
        "\n",
        "        loss = cross_entropy_loss(X, y, weights)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Check for convergence based on the magnitude of the update step\n",
        "        if np.linalg.norm(update) < tolerance:\n",
        "            print(f\"Convergence reached at iteration {i}.\")\n",
        "            break\n",
        "\n",
        "        # Optional: print progress\n",
        "        # if (i + 1) % 10 == 0:\n",
        "        #     print(f\"Iteration {i+1}, Loss: {loss}\")\n",
        "\n",
        "    print(f\"Finished after {i+1} iterations.\")\n",
        "    return weights, loss_history\n",
        "\n",
        "# 9. Train the logistic regression model using the implemented Newton's method\n",
        "# Initialize weights with zeros (including for the bias term)\n",
        "initial_weights = np.zeros(X_train.shape[1])\n",
        "\n",
        "print(\"\\nStarting Newton's method training...\")\n",
        "optimal_weights, history = newtons_method(X_train, y_train, initial_weights)\n",
        "\n",
        "print(\"\\nOptimal weights found:\")\n",
        "print(optimal_weights)\n",
        "\n",
        "# 10. Evaluate the trained model\n",
        "# Make predictions on the training data\n",
        "# The predict function gives probabilities, convert to class labels (0 or 1)\n",
        "predicted_probabilities = predict(X_train, optimal_weights)\n",
        "predicted_classes = (predicted_probabilities > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nModel Evaluation on Training Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_train, predicted_classes))\n",
        "print(\"Precision:\", precision_score(y_train, predicted_classes))\n",
        "print(\"Recall:\", recall_score(y_train, predicted_classes))\n",
        "print(\"F1-score:\", f1_score(y_train, predicted_classes))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Newton's method training...\n",
            "Finished after 100 iterations.\n",
            "\n",
            "Optimal weights found:\n",
            "[ -5.85966636 -25.18595097 -12.49249928  45.50349927  59.98835501]\n",
            "\n",
            "Model Evaluation on Training Data:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70202c26"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Newton's method is not directly applicable to classification problems because classification performance metrics are not continuous or differentiable functions.\n",
        "*   To apply Newton's method to classification, the problem must be reframed as an optimization task with a continuous and differentiable objective function, such as minimizing the cross-entropy loss in Logistic Regression.\n",
        "*   The cross-entropy loss function in Logistic Regression is differentiable with respect to the model parameters, allowing for the calculation of the gradient and Hessian required by Newton's method.\n",
        "*   An assignment was outlined detailing the steps to implement Logistic Regression with Newton's method on a binary subset of the IRIS dataset.\n",
        "*   An example Python implementation demonstrated:\n",
        "    *   Loading and preparing a binary subset of the IRIS dataset (setosa vs. versicolor), including adding a bias term and converting labels to 0 and 1.\n",
        "    *   Implementing the core Logistic Regression components: sigmoid function, prediction function, cross-entropy loss, gradient, and Hessian.\n",
        "    *   Implementing Newton's method for optimization, utilizing the pseudo-inverse of the Hessian for stability.\n",
        "    *   Training the model using the implemented Newton's method.\n",
        "    *   Evaluating the model on the training data, which resulted in perfect scores (1.0) for Accuracy, Precision, Recall, and F1-score for the setosa vs. versicolor task, consistent with the known linear separability of these classes.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The implementation successfully demonstrates how Newton's method can be used to optimize the parameters of a Logistic Regression model by minimizing a differentiable loss function.\n",
        "*   For future work, the user could extend this implementation to handle multi-class classification (e.g., using a One-vs-Rest strategy with Logistic Regression) and explore regularization techniques within the Newton's method framework to prevent overfitting on more complex datasets.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}