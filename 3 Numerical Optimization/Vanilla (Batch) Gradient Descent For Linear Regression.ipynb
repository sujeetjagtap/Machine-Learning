{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmK7HzuzSBVb"
      },
      "source": [
        "Let's start with the Linear Regression Cost Function: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{i} - y^{i})^{2}$.\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{i} - y^{i})\\cdot x_{j}^{i}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JsvR8aCSBVe"
      },
      "source": [
        "For Vanilla (Batch) Gradient Descent:\n",
        "    $\\theta_{j} := \\theta_{j} - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) = \\theta_{j} - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{i} - y^{i})\\cdot x_{j}^{i}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDHoIb1ZSBVf"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhq75FMwSBVh"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(X,y,theta,alpha,num_iters):\n",
        "    \"\"\" Performs gradient descent to learn theta\"\"\"\n",
        "\n",
        "    # number of training examples,\n",
        "    # neat trick: y.size = np.prod(y.shape)\n",
        "    m = y.size\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(X, theta)\n",
        "        theta = theta - alpha*(1.0/m) * np.dot(X.T, y_hat-y)\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsY7CTm8SBVi"
      },
      "source": [
        "Notice the $\\textbf{np.dot(X.T, y_hat-y)}$ above? That's basically a vectorized version of \"looping through (summing) the number of training samples\". YES, it takes a lot of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61a0096"
      },
      "source": [
        "**Explanation of `gradientDescent` function:**\n",
        "\n",
        "The `gradientDescent` function implements the batch gradient descent algorithm for linear regression. It takes the feature matrix `X`, target variable vector `y`, initial parameter vector `theta`, learning rate `alpha`, and number of iterations `num_iters` as input.\n",
        "\n",
        "The function iteratively updates the parameter vector `theta` using the following formula:\n",
        "$\\theta_{j} := \\theta_{j} - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{i} - y^{i})\\cdot x_{j}^{i}$\n",
        "\n",
        "Inside the loop, it calculates the predicted values `y_hat`, computes the gradient of the cost function using the vectorized form `np.dot(X.T, y_hat-y)`, and updates `theta` by subtracting the scaled gradient. This process is repeated for the specified number of iterations to minimize the cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d713e12"
      },
      "source": [
        "Let's break down the `gradientDescent` function in more detail:\n",
        "\n",
        "**`def gradientDescent(X,y,theta,alpha,num_iters):`**\n",
        "This line defines the function and its parameters:\n",
        "- `X`: This is a NumPy array representing the feature matrix. Each row corresponds to a training example, and each column represents a feature.\n",
        "- `y`: This is a NumPy array representing the target variable vector. Each element is the target value for the corresponding training example in `X`.\n",
        "- `theta`: This is a NumPy array representing the parameter vector (weights) of the linear regression model. It is initialized before calling the function and updated during the gradient descent process.\n",
        "- `alpha`: This is a scalar value representing the learning rate. It controls how large of a step the algorithm takes in the direction of the negative gradient during each iteration. A smaller `alpha` leads to slower convergence but can prevent overshooting the minimum. A larger `alpha` can lead to faster convergence but may overshoot or even diverge.\n",
        "- `num_iters`: This is an integer specifying the number of iterations the gradient descent algorithm will run.\n",
        "\n",
        "**`m = y.size`**\n",
        "This line calculates the number of training examples `m`. `y.size` returns the total number of elements in the `y` array, which is the number of training examples.\n",
        "\n",
        "**`for i in range(num_iters):`**\n",
        "This is the main loop of the gradient descent algorithm. It iterates `num_iters` times, performing one update of the parameter vector `theta` in each iteration.\n",
        "\n",
        "**`y_hat = np.dot(X, theta)`**\n",
        "Inside the loop, this line calculates the predicted values `y_hat` for all training examples. This is done using matrix multiplication:\n",
        "- `X` has dimensions (m, n), where m is the number of training examples and n is the number of features.\n",
        "- `theta` has dimensions (n, 1).\n",
        "- `np.dot(X, theta)` results in a vector `y_hat` with dimensions (m, 1), where each element is the predicted value for the corresponding training example. This is the vectorized form of $\\hat{y}^{(i)} = \\theta^T x^{(i)}$.\n",
        "\n",
        "**`theta = theta - alpha*(1.0/m) * np.dot(X.T, y_hat-y)`**\n",
        "This is the crucial line where the parameter vector `theta` is updated. Let's break it down:\n",
        "- `y_hat-y`: This calculates the error for each training example by subtracting the actual target values `y` from the predicted values `y_hat`. The result is a vector with dimensions (m, 1).\n",
        "- `X.T`: This is the transpose of the feature matrix `X`. It has dimensions (n, m).\n",
        "- `np.dot(X.T, y_hat-y)`: This calculates the gradient of the cost function with respect to `theta`. It's the vectorized form of the summation $\\sum_{i=1}^{m} (\\hat{y}^{i} - y^{i})\\cdot x_{j}^{i}$ for each parameter $\\theta_j$. The result is a vector with dimensions (n, 1), where each element is the partial derivative of the cost function with respect to the corresponding parameter.\n",
        "- `(1.0/m)`: This term averages the gradient over all training examples.\n",
        "- `alpha*(1.0/m) * np.dot(X.T, y_hat-y)`: This calculates the step size and direction for updating `theta`. It's the scaled gradient.\n",
        "- `theta - ...`: This subtracts the scaled gradient from the current `theta` to get the new, updated `theta`. This moves `theta` in the direction that reduces the cost function.\n",
        "\n",
        "**`return theta`**\n",
        "After the loop completes, the function returns the final, optimized parameter vector `theta`. These parameters can then be used to make predictions on new, unseen data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}